{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import spacy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../final_data.csv', encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle dataset and split into train, valoidation and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'IMDB Score': 'imdb_score'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['posterID', 'Genre', 'overview', 'director', 'actors',\n",
    "       'runtime', 'num_faces', 'brightness', 'saturation', 'hue',\n",
    "       'brightness_sd', 'saturation_sd ', 'hue_sd', 'blue', 'blue_sd', 'green',\n",
    "       'green_sd', 'red', 'red_sd ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(data, train_size=0.7):\n",
    "    X_train, RX = train_test_split(data, test_size=1-train_size, random_state=0)\n",
    "    X_val, X_test = train_test_split(RX, test_size=0.5, random_state=0)\n",
    "    \n",
    "    return X_train, X_val, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test = split_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('../data/X_train.csv', index=False)\n",
    "X_val.to_csv('../data/X_val.csv', index=False)\n",
    "X_test.to_csv('../data/X_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('../data/X_train.csv', encoding='latin1')\n",
    "X_val = pd.read_csv('../data/X_val.csv', encoding='latin1')\n",
    "X_test = pd.read_csv('../data/X_test.csv', encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_words(s, tokenizer=None):\n",
    "    if tokenizer:\n",
    "        words = tokenizer(s)\n",
    "        return [i.text for i in words]\n",
    "    else:\n",
    "        return s.lower().split('|')\n",
    "\n",
    "def get_dict(data, min_count=5, tokenizer=None):\n",
    "    count = {}\n",
    "    for s in data:\n",
    "        words = split_words(s, tokenizer)\n",
    "        for word in set(words):\n",
    "            if word in count:\n",
    "                count[word] += 1\n",
    "            else:\n",
    "                count[word] = 1\n",
    "                \n",
    "    for word in list(count.keys()):\n",
    "        if count[word] < min_count:\n",
    "            del count[word]\n",
    "            \n",
    "    for i, word in enumerate(count.keys()):\n",
    "        count[word] = i\n",
    "    \n",
    "    return count\n",
    "\n",
    "def transform_text(data, word_count, tokenizer= None):\n",
    "    tr = np.zeros((len(data), len(word_count)))\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        words = split_words(data[i], tokenizer)\n",
    "        for word in words:\n",
    "            if word in word_count:\n",
    "                tr[i][word_count[word]] += 1\n",
    "            \n",
    "    return tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_count = get_dict(X_train.Genre.astype(str))\n",
    "\n",
    "genre_train = transform_text(X_train.Genre, genre_count)\n",
    "genre_val = transform_text(X_val.Genre, genre_count)\n",
    "genre_test = transform_text(X_test.Genre, genre_count)\n",
    "\n",
    "print(genre_train.shape)\n",
    "print(genre_val.shape)\n",
    "print(genre_test.shape)\n",
    "print(len(genre_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_count = get_dict(X_train.director.astype(str))\n",
    "\n",
    "dir_train = transform_text(X_train.director, dir_count)\n",
    "dir_val = transform_text(X_val.director, dir_count)\n",
    "dir_test = transform_text(X_test.director, dir_count)\n",
    "\n",
    "print(dir_train.shape)\n",
    "print(dir_val.shape)\n",
    "print(dir_test.shape)\n",
    "print(len(dir_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_count = get_dict(X_train.actors.astype(str))\n",
    "\n",
    "actor_train = transform_text(X_train.actors, actor_count)\n",
    "actor_val = transform_text(X_val.actors, actor_count)\n",
    "actor_test = transform_text(X_test.actors, actor_count)\n",
    "\n",
    "print(actor_train.shape)\n",
    "print(actor_val.shape)\n",
    "print(actor_test.shape)\n",
    "print(len(actor_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"ner\"])\n",
    "\n",
    "overview_count = get_dict(X_train.overview.astype(str), min_count=20, tokenizer=tokenizer)\n",
    "\n",
    "overview_train = transform_text(X_train.overview, overview_count, tokenizer=tokenizer)\n",
    "overview_val = transform_text(X_val.overview, overview_count, tokenizer=tokenizer)\n",
    "overview_test = transform_text(X_test.overview, overview_count, tokenizer=tokenizer)\n",
    "\n",
    "print(overview_train.shape)\n",
    "print(overview_val.shape)\n",
    "print(overview_test.shape)\n",
    "print(len(overview_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data =  pd.concat([X_train, pd.DataFrame(genre_train, columns=genre_count.keys()),\n",
    "          pd.DataFrame(dir_train, columns=dir_count.keys()),\n",
    "          pd.DataFrame(actor_train, columns=actor_count.keys()),\n",
    "          pd.DataFrame(overview_train, columns=overview_count.keys())], axis = 1)\n",
    "\n",
    "val_data =  pd.concat([X_val, pd.DataFrame(genre_val, columns=genre_count.keys()),\n",
    "          pd.DataFrame(dir_val, columns=dir_count.keys()),\n",
    "          pd.DataFrame(actor_val, columns=actor_count.keys()),\n",
    "          pd.DataFrame(overview_val, columns=overview_count.keys())], axis = 1)\n",
    "\n",
    "test_data =  pd.concat([X_test, pd.DataFrame(genre_test, columns=genre_count.keys()),\n",
    "          pd.DataFrame(dir_test, columns=dir_count.keys()),\n",
    "          pd.DataFrame(actor_test, columns=actor_count.keys()),\n",
    "          pd.DataFrame(overview_test, columns=overview_count.keys())], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop(columns=['Genre', 'overview', 'director', 'actors'])\n",
    "val_data = val_data.drop(columns=['Genre', 'overview', 'director', 'actors'])\n",
    "test_data = test_data.drop(columns=['Genre', 'overview', 'director', 'actors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('../data/train_final.csv', index=False)\n",
    "val_data.to_csv('../data/val_final.csv', index=False)\n",
    "test_data.to_csv('../data/test_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../data/train_final.csv', encoding='latin1')\n",
    "val_data = pd.read_csv('../data/val_final.csv', encoding='latin1')\n",
    "test_data = pd.read_csv('../data/test_final.csv', encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13194 entries, 0 to 13193\n",
      "Columns: 5910 entries, posterID to eliminate\n",
      "dtypes: float64(5909), object(1)\n",
      "memory usage: 594.9+ MB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['runtime','num_faces','hue','hue_sd','saturation','saturation_sd ','brightness','brightness_sd',\n",
    "            'blue','blue_sd','green','green_sd','red','red_sd ']:\n",
    "    mean_feature = np.mean(train_data[col])\n",
    "    sd_feature = np.std(train_data[col])\n",
    "    \n",
    "    if sd_feature != 0:\n",
    "        train_data[col] = (train_data[col] - mean_feature) / sd_feature\n",
    "        val_data[col] = (val_data[col] - mean_feature) / sd_feature\n",
    "        test_data[col] = (test_data[col] - mean_feature) / sd_feature\n",
    "    else:\n",
    "        train_data[col] = (train_data[col] - mean_feature) \n",
    "        val_data[col] = (val_data[col] - mean_feature) \n",
    "        test_data[col] = (test_data[col] - mean_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posterID</th>\n",
       "      <th>imdb_score</th>\n",
       "      <th>runtime</th>\n",
       "      <th>num_faces</th>\n",
       "      <th>brightness</th>\n",
       "      <th>saturation</th>\n",
       "      <th>hue</th>\n",
       "      <th>brightness_sd</th>\n",
       "      <th>saturation_sd</th>\n",
       "      <th>hue_sd</th>\n",
       "      <th>...</th>\n",
       "      <th>subway</th>\n",
       "      <th>cities</th>\n",
       "      <th>excitement</th>\n",
       "      <th>monstrous</th>\n",
       "      <th>May</th>\n",
       "      <th>traps</th>\n",
       "      <th>traffic</th>\n",
       "      <th>heir</th>\n",
       "      <th>fearless</th>\n",
       "      <th>eliminate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>118747.jpg</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0.783686</td>\n",
       "      <td>-0.216658</td>\n",
       "      <td>-0.211632</td>\n",
       "      <td>-0.832558</td>\n",
       "      <td>2.121743</td>\n",
       "      <td>1.853820</td>\n",
       "      <td>-0.387582</td>\n",
       "      <td>-0.339980</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118002.jpg</td>\n",
       "      <td>5.2</td>\n",
       "      <td>-0.044494</td>\n",
       "      <td>-0.216658</td>\n",
       "      <td>-1.197686</td>\n",
       "      <td>-0.371092</td>\n",
       "      <td>-0.279790</td>\n",
       "      <td>-0.896663</td>\n",
       "      <td>-0.498212</td>\n",
       "      <td>0.927495</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>245407.jpg</td>\n",
       "      <td>5.9</td>\n",
       "      <td>0.446279</td>\n",
       "      <td>-0.216658</td>\n",
       "      <td>1.803157</td>\n",
       "      <td>-0.938981</td>\n",
       "      <td>-0.561388</td>\n",
       "      <td>-1.358654</td>\n",
       "      <td>-0.300175</td>\n",
       "      <td>0.476504</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101702.jpg</td>\n",
       "      <td>2.7</td>\n",
       "      <td>-0.013821</td>\n",
       "      <td>-0.216658</td>\n",
       "      <td>0.653993</td>\n",
       "      <td>-0.312038</td>\n",
       "      <td>-0.672706</td>\n",
       "      <td>0.982971</td>\n",
       "      <td>0.531417</td>\n",
       "      <td>0.350392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4717798.jpg</td>\n",
       "      <td>6.5</td>\n",
       "      <td>-0.565941</td>\n",
       "      <td>-0.216658</td>\n",
       "      <td>-0.004048</td>\n",
       "      <td>0.907230</td>\n",
       "      <td>-0.093724</td>\n",
       "      <td>0.017742</td>\n",
       "      <td>0.753552</td>\n",
       "      <td>0.472705</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13189</th>\n",
       "      <td>453508.jpg</td>\n",
       "      <td>8.6</td>\n",
       "      <td>-0.504594</td>\n",
       "      <td>-0.216658</td>\n",
       "      <td>-0.038594</td>\n",
       "      <td>0.960138</td>\n",
       "      <td>-0.934452</td>\n",
       "      <td>-0.733416</td>\n",
       "      <td>-0.928683</td>\n",
       "      <td>-1.157100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13190</th>\n",
       "      <td>197256.jpg</td>\n",
       "      <td>5.7</td>\n",
       "      <td>0.231566</td>\n",
       "      <td>-0.216658</td>\n",
       "      <td>-0.714362</td>\n",
       "      <td>-0.840712</td>\n",
       "      <td>0.235713</td>\n",
       "      <td>1.616463</td>\n",
       "      <td>0.784342</td>\n",
       "      <td>0.747825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13191</th>\n",
       "      <td>1534084.jpg</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.078199</td>\n",
       "      <td>-0.216658</td>\n",
       "      <td>0.408528</td>\n",
       "      <td>-0.926015</td>\n",
       "      <td>-1.103991</td>\n",
       "      <td>-0.203606</td>\n",
       "      <td>-0.973200</td>\n",
       "      <td>-1.619233</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13192</th>\n",
       "      <td>1621446.jpg</td>\n",
       "      <td>4.3</td>\n",
       "      <td>-0.228534</td>\n",
       "      <td>-0.216658</td>\n",
       "      <td>-1.272881</td>\n",
       "      <td>-0.823072</td>\n",
       "      <td>0.742440</td>\n",
       "      <td>0.644669</td>\n",
       "      <td>-0.088083</td>\n",
       "      <td>-0.180132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13193</th>\n",
       "      <td>93036.jpg</td>\n",
       "      <td>5.7</td>\n",
       "      <td>-0.136514</td>\n",
       "      <td>-0.216658</td>\n",
       "      <td>-0.252636</td>\n",
       "      <td>1.188506</td>\n",
       "      <td>0.603484</td>\n",
       "      <td>1.346960</td>\n",
       "      <td>0.914660</td>\n",
       "      <td>0.511679</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13194 rows Ã— 5910 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          posterID  imdb_score   runtime  num_faces  brightness  saturation  \\\n",
       "0       118747.jpg         4.8  0.783686  -0.216658   -0.211632   -0.832558   \n",
       "1       118002.jpg         5.2 -0.044494  -0.216658   -1.197686   -0.371092   \n",
       "2       245407.jpg         5.9  0.446279  -0.216658    1.803157   -0.938981   \n",
       "3       101702.jpg         2.7 -0.013821  -0.216658    0.653993   -0.312038   \n",
       "4      4717798.jpg         6.5 -0.565941  -0.216658   -0.004048    0.907230   \n",
       "...            ...         ...       ...        ...         ...         ...   \n",
       "13189   453508.jpg         8.6 -0.504594  -0.216658   -0.038594    0.960138   \n",
       "13190   197256.jpg         5.7  0.231566  -0.216658   -0.714362   -0.840712   \n",
       "13191  1534084.jpg         4.5  0.078199  -0.216658    0.408528   -0.926015   \n",
       "13192  1621446.jpg         4.3 -0.228534  -0.216658   -1.272881   -0.823072   \n",
       "13193    93036.jpg         5.7 -0.136514  -0.216658   -0.252636    1.188506   \n",
       "\n",
       "            hue  brightness_sd  saturation_sd     hue_sd  ...  subway  cities  \\\n",
       "0      2.121743       1.853820       -0.387582 -0.339980  ...     0.0     0.0   \n",
       "1     -0.279790      -0.896663       -0.498212  0.927495  ...     0.0     0.0   \n",
       "2     -0.561388      -1.358654       -0.300175  0.476504  ...     0.0     0.0   \n",
       "3     -0.672706       0.982971        0.531417  0.350392  ...     0.0     0.0   \n",
       "4     -0.093724       0.017742        0.753552  0.472705  ...     0.0     0.0   \n",
       "...         ...            ...             ...       ...  ...     ...     ...   \n",
       "13189 -0.934452      -0.733416       -0.928683 -1.157100  ...     0.0     0.0   \n",
       "13190  0.235713       1.616463        0.784342  0.747825  ...     0.0     0.0   \n",
       "13191 -1.103991      -0.203606       -0.973200 -1.619233  ...     0.0     0.0   \n",
       "13192  0.742440       0.644669       -0.088083 -0.180132  ...     0.0     0.0   \n",
       "13193  0.603484       1.346960        0.914660  0.511679  ...     0.0     0.0   \n",
       "\n",
       "       excitement  monstrous  May  traps  traffic  heir  fearless  eliminate  \n",
       "0             0.0        0.0  0.0    0.0      0.0   0.0       0.0        0.0  \n",
       "1             0.0        0.0  0.0    0.0      0.0   0.0       0.0        0.0  \n",
       "2             0.0        0.0  0.0    0.0      0.0   0.0       0.0        0.0  \n",
       "3             0.0        0.0  0.0    0.0      0.0   0.0       0.0        0.0  \n",
       "4             0.0        0.0  0.0    0.0      0.0   0.0       0.0        0.0  \n",
       "...           ...        ...  ...    ...      ...   ...       ...        ...  \n",
       "13189         0.0        0.0  0.0    0.0      0.0   0.0       0.0        0.0  \n",
       "13190         0.0        0.0  0.0    0.0      0.0   0.0       0.0        0.0  \n",
       "13191         0.0        0.0  0.0    0.0      0.0   0.0       0.0        0.0  \n",
       "13192         0.0        0.0  0.0    0.0      0.0   0.0       0.0        0.0  \n",
       "13193         0.0        0.0  0.0    0.0      0.0   0.0       0.0        0.0  \n",
       "\n",
       "[13194 rows x 5910 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4771817495212598\n",
      "1.4264534852595883\n",
      "1.4920414168987786\n"
     ]
    }
   ],
   "source": [
    "print(np.cov(train_data.imdb_score))\n",
    "print(np.cov(val_data.imdb_score))\n",
    "print(np.cov(test_data.imdb_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_data.drop(columns=['imdb_score', 'posterID'])\n",
    "val_X = val_data.drop(columns=['imdb_score', 'posterID'])\n",
    "test_X = test_data.drop(columns=['imdb_score', 'posterID'])\n",
    "train_y = train_data.imdb_score\n",
    "val_y = val_data.imdb_score\n",
    "test_y = test_data.imdb_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train r^2:  0.7043745138751877\n",
      "train mse:  0.4366594749817954\n",
      "valid r^2:  0.005851745190784463\n",
      "valid mse:  1.4176047909419611\n",
      "test r^2:  -5.173955098096975e+18\n",
      "test mse:  7.717025537651409e+18\n"
     ]
    }
   ],
   "source": [
    "reg = LinearRegression().fit(train_X, train_y)\n",
    "\n",
    "print('train r^2: ', reg.score(train_X, train_y))\n",
    "print('train mse: ', mean_squared_error(reg.predict(train_X), train_y))\n",
    "\n",
    "print('valid r^2: ', reg.score(val_X, val_y))\n",
    "print('valid mse: ', mean_squared_error(reg.predict(val_X), val_y))\n",
    "\n",
    "print('test r^2: ', reg.score(test_X, test_y))\n",
    "print('test mse: ', mean_squared_error(reg.predict(test_X), test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train r^2:  0.6389160207073561\n",
      "train mse:  0.5333462377993188\n",
      "valid r^2:  0.28712903556613334\n",
      "valid mse:  1.016517697049925\n",
      "test r^2:  0.34131220992775924\n",
      "test mse:  0.9824419425665801\n"
     ]
    }
   ],
   "source": [
    "ridgeReg = Ridge(alpha = 10, random_state = 0).fit(train_X, train_y)\n",
    "\n",
    "print('train r^2: ', ridgeReg.score(train_X, train_y))\n",
    "print('train mse: ', mean_squared_error(ridgeReg.predict(train_X), train_y))\n",
    "\n",
    "print('valid r^2: ', ridgeReg.score(val_X, val_y))\n",
    "print('valid mse: ', mean_squared_error(ridgeReg.predict(val_X), val_y))\n",
    "\n",
    "print('test r^2: ', ridgeReg.score(test_X, test_y))\n",
    "print('test mse: ', mean_squared_error(ridgeReg.predict(test_X), test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classified to the nearest pt in {1, 3, 5, 7, 9}\n",
    "def to_5_classes(y): \n",
    "    classes = np.ceil(y/2)\n",
    "    return classes\n",
    "\n",
    "def cls_5_to_value(cls):\n",
    "    new_y = cls * 2 - 1\n",
    "    return new_y\n",
    "\n",
    "# classified to the nearest pt in {0.5, 1.5, 2.5, ... , 8.5, 9.5}\n",
    "def to_10_classes(y):\n",
    "    classes = np.ceil(y)\n",
    "    return classes\n",
    "\n",
    "def cls_10_to_value(cls):\n",
    "    new_y = cls * 1 - 0.5\n",
    "    return new_y\n",
    "  \n",
    "# classified to the nearest pt in {0.25, 0.75, 1.25, 1.75, ... , 9.25, 9.75}\n",
    "def to_20_classes(y):\n",
    "    classes = np.ceil(y*2)\n",
    "    return classes\n",
    "\n",
    "def cls_20_to_value(cls):\n",
    "    new_y = cls * (1/2) - 0.25\n",
    "    return new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_5_cls = train_y.apply(to_5_classes)\n",
    "train_y_10_cls = train_y.apply(to_10_classes)\n",
    "train_y_20_cls = train_y.apply(to_20_classes)\n",
    "\n",
    "val_y_5_cls = val_y.apply(to_5_classes)\n",
    "val_y_10_cls = val_y.apply(to_10_classes)\n",
    "val_y_20_cls = val_y.apply(to_20_classes)\n",
    "\n",
    "test_y_5_cls = test_y.apply(to_5_classes)\n",
    "test_y_10_cls = test_y.apply(to_10_classes)\n",
    "test_y_20_cls = test_y.apply(to_20_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.8614521752311657\n",
      "Train MSE:  0.6599969683189328\n",
      "Valid accuracy:  0.5753182461103253\n",
      "Valid MSE:  2.113154172560113\n",
      "Test accuracy:  0.5823903818953324\n",
      "Test MSE:  2.0438472418670437\n"
     ]
    }
   ],
   "source": [
    "logReg_5_cls = LogisticRegression(penalty='none', solver='lbfgs',multi_class='multinomial').fit(train_X, train_y_5_cls)\n",
    "\n",
    "train_y_5_pred = np.apply_along_axis(cls_5_to_value, 0, logReg_5_cls.predict(train_X))\n",
    "val_y_5_pred = np.apply_along_axis(cls_5_to_value, 0, logReg_5_cls.predict(val_X))\n",
    "test_y_5_pred = np.apply_along_axis(cls_5_to_value, 0, logReg_5_cls.predict(test_X))\n",
    "\n",
    "print('Train accuracy: ', logReg_5_cls.score(train_X, train_y_5_cls))\n",
    "print('Train MSE: ', mean_squared_error(train_y_5_pred, train_y_5_cls.apply(cls_5_to_value)))\n",
    "\n",
    "print('Valid accuracy: ', logReg_5_cls.score(val_X, val_y_5_cls))\n",
    "print('Valid MSE: ', mean_squared_error(val_y_5_pred, val_y_5_cls.apply(cls_5_to_value)))\n",
    "\n",
    "print('Test accuracy: ', logReg_5_cls.score(test_X, test_y_5_cls))\n",
    "print('Test MSE: ', mean_squared_error(test_y_5_pred, test_y_5_cls.apply(cls_5_to_value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grigor/anaconda3/envs/work/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:760: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.7051690162194937\n",
      "Train MSE:  0.7668637259360316\n",
      "Valid accuracy:  0.33734087694483733\n",
      "Valid MSE:  1.5792079207920793\n",
      "Test accuracy:  0.3497171145685997\n",
      "Test MSE:  1.5484441301272984\n"
     ]
    }
   ],
   "source": [
    "logReg_10_cls = LogisticRegression(penalty='none', solver='lbfgs',multi_class='multinomial').fit(train_X, train_y_10_cls)\n",
    "\n",
    "train_y_10_pred = np.apply_along_axis(cls_10_to_value, 0, logReg_10_cls.predict(train_X))\n",
    "val_y_10_pred = np.apply_along_axis(cls_10_to_value, 0, logReg_10_cls.predict(val_X))\n",
    "test_y_10_pred = np.apply_along_axis(cls_10_to_value, 0, logReg_10_cls.predict(test_X))\n",
    "\n",
    "print('Train accuracy: ', logReg_10_cls.score(train_X, train_y_10_cls))\n",
    "print('Train MSE: ', mean_squared_error(train_y_10_pred, train_y_10_cls.apply(cls_10_to_value)))\n",
    "\n",
    "print('Valid accuracy: ', logReg_10_cls.score(val_X, val_y_10_cls))\n",
    "print('Valid MSE: ', mean_squared_error(val_y_10_pred, val_y_10_cls.apply(cls_10_to_value)))\n",
    "\n",
    "print('Test accuracy: ', logReg_10_cls.score(test_X, test_y_10_cls))\n",
    "print('Test MSE: ', mean_squared_error(test_y_10_pred, test_y_10_cls.apply(cls_10_to_value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grigor/anaconda3/envs/work/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:760: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.6414279217826284\n",
      "Train MSE:  0.7121608306806124\n",
      "Valid accuracy:  0.1934229137199434\n",
      "Valid MSE:  1.4272454031117396\n",
      "Test accuracy:  0.18705799151343705\n",
      "Test MSE:  1.458981612446959\n"
     ]
    }
   ],
   "source": [
    "logReg_20_cls = LogisticRegression(penalty='none', solver='lbfgs',multi_class='multinomial').fit(train_X, train_y_20_cls)\n",
    "\n",
    "train_y_20_pred = np.apply_along_axis(cls_20_to_value, 0, logReg_20_cls.predict(train_X))\n",
    "val_y_20_pred = np.apply_along_axis(cls_20_to_value, 0, logReg_20_cls.predict(val_X))\n",
    "test_y_20_pred = np.apply_along_axis(cls_20_to_value, 0, logReg_20_cls.predict(test_X))\n",
    "\n",
    "print('Train accuracy: ', logReg_20_cls.score(train_X, train_y_20_cls))\n",
    "print('Train MSE: ', mean_squared_error(train_y_20_pred, train_y_20_cls.apply(cls_20_to_value)))\n",
    "\n",
    "print('Valid accuracy: ', logReg_20_cls.score(val_X, val_y_20_cls))\n",
    "print('Valid MSE: ', mean_squared_error(val_y_20_pred, val_y_20_cls.apply(cls_20_to_value)))\n",
    "\n",
    "print('Test accuracy: ', logReg_20_cls.score(test_X, test_y_20_cls))\n",
    "print('Test MSE: ', mean_squared_error(test_y_20_pred, test_y_20_cls.apply(cls_20_to_value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grigor/anaconda3/envs/work/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:760: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.8009701379414885\n",
      "Train MSE:  0.9507351826587843\n",
      "Valid accuracy:  0.6145685997171145\n",
      "Valid MSE:  1.7736916548797736\n",
      "Test accuracy:  0.632956152758133\n",
      "Test MSE:  1.7001414427157002\n"
     ]
    }
   ],
   "source": [
    "# Regularization c = 1/alpha = 0.1\n",
    "logRegL2_5_cls = LogisticRegression(penalty='l2', C = 0.1, solver='lbfgs',multi_class='multinomial').fit(train_X, train_y_5_cls)\n",
    "\n",
    "train_y_5_pred = np.apply_along_axis(cls_5_to_value, 0, logRegL2_5_cls.predict(train_X))\n",
    "val_y_5_pred = np.apply_along_axis(cls_5_to_value, 0, logRegL2_5_cls.predict(val_X))\n",
    "test_y_5_pred = np.apply_along_axis(cls_5_to_value, 0, logRegL2_5_cls.predict(test_X))\n",
    "\n",
    "print('Train accuracy: ', logRegL2_5_cls.score(train_X, train_y_5_cls))\n",
    "print('Train MSE: ', mean_squared_error(train_y_5_pred, train_y_5_cls.apply(cls_5_to_value)))\n",
    "\n",
    "print('Valid accuracy: ', logRegL2_5_cls.score(val_X, val_y_5_cls))\n",
    "print('Valid MSE: ', mean_squared_error(val_y_5_pred, val_y_5_cls.apply(cls_5_to_value)))\n",
    "\n",
    "print('Test accuracy: ', logRegL2_5_cls.score(test_X, test_y_5_cls))\n",
    "print('Test MSE: ', mean_squared_error(test_y_5_pred, test_y_5_cls.apply(cls_5_to_value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grigor/anaconda3/envs/work/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:760: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.6566621191450659\n",
      "Train MSE:  0.8618311353645597\n",
      "Valid accuracy:  0.3734087694483734\n",
      "Valid MSE:  1.3454738330975955\n",
      "Test accuracy:  0.38613861386138615\n",
      "Test MSE:  1.3426449787835926\n"
     ]
    }
   ],
   "source": [
    "# Regularization c = 1/alpha = 0.1\n",
    "logRegL2_10_cls = LogisticRegression(penalty='l2', C = 0.1, solver='lbfgs',multi_class='multinomial').fit(train_X, train_y_10_cls)\n",
    "\n",
    "train_y_10_pred = np.apply_along_axis(cls_10_to_value, 0, logRegL2_10_cls.predict(train_X))\n",
    "val_y_10_pred = np.apply_along_axis(cls_10_to_value, 0, logRegL2_10_cls.predict(val_X))\n",
    "test_y_10_pred = np.apply_along_axis(cls_10_to_value, 0, logRegL2_10_cls.predict(test_X))\n",
    "\n",
    "print('Train accuracy: ', logRegL2_10_cls.score(train_X, train_y_10_cls))\n",
    "print('Train MSE: ', mean_squared_error(train_y_10_pred, train_y_10_cls.apply(cls_10_to_value)))\n",
    "\n",
    "print('Valid accuracy: ', logRegL2_10_cls.score(val_X, val_y_10_cls))\n",
    "print('Valid MSE: ', mean_squared_error(val_y_10_pred, val_y_10_cls.apply(cls_10_to_value)))\n",
    "\n",
    "print('Test accuracy: ', logRegL2_10_cls.score(test_X, test_y_10_cls))\n",
    "print('Test MSE: ', mean_squared_error(test_y_10_pred, test_y_10_cls.apply(cls_10_to_value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grigor/anaconda3/envs/work/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:760: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.5842049416401395\n",
      "Train MSE:  0.806976656055783\n",
      "Valid accuracy:  0.20862800565770862\n",
      "Valid MSE:  1.2661775106082036\n",
      "Test accuracy:  0.20332390381895332\n",
      "Test MSE:  1.2583097595473833\n"
     ]
    }
   ],
   "source": [
    "# Regularization c = 1/alpha = 0.1\n",
    "logRegL2_20_cls = LogisticRegression(penalty='l2', C = 0.1, solver='lbfgs',multi_class='multinomial').fit(train_X, train_y_20_cls)\n",
    "\n",
    "train_y_20_pred = np.apply_along_axis(cls_20_to_value, 0, logRegL2_20_cls.predict(train_X))\n",
    "val_y_20_pred = np.apply_along_axis(cls_20_to_value, 0, logRegL2_20_cls.predict(val_X))\n",
    "test_y_20_pred = np.apply_along_axis(cls_20_to_value, 0, logRegL2_20_cls.predict(test_X))\n",
    "\n",
    "print('Train accuracy: ', logRegL2_20_cls.score(train_X, train_y_20_cls))\n",
    "print('Train MSE: ', mean_squared_error(train_y_20_pred, train_y_20_cls.apply(cls_20_to_value)))\n",
    "\n",
    "print('Valid accuracy: ', logRegL2_20_cls.score(val_X, val_y_20_cls))\n",
    "print('Valid MSE: ', mean_squared_error(val_y_20_pred, val_y_20_cls.apply(cls_20_to_value)))\n",
    "\n",
    "print('Test accuracy: ', logRegL2_20_cls.score(test_X, test_y_20_cls))\n",
    "print('Test MSE: ', mean_squared_error(test_y_20_pred, test_y_20_cls.apply(cls_20_to_value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression vs Logistic Regression (without regularization)\n",
      "Test R^2\n",
      "Linear Reg: -5.173955098096975e+18\n",
      "Log Reg (5 cls): -0.18030078679410133\n",
      "Log Reg (10 cls): 0.023373888145752808\n",
      "Log Reg (20 cls): 0.0374326801652306\n",
      "Test Accuracy\n",
      "Linear Reg (5 cls): 0.5721357850070722\n",
      "Log Reg (5 cls): 0.5823903818953324\n",
      "Linear Reg (10 cls): 0.326025459688826\n",
      "Log Reg (10 cls): 0.3497171145685997\n",
      "Linear Reg (20 cls): 0.1637199434229137\n",
      "Log Reg (20 cls): 0.18705799151343705\n",
      "Test MSE\n",
      "Linear Reg: 7.717025537651409e+18\n",
      "Log Reg (5 cls): 2.0438472418670437\n",
      "Log Reg (10 cls): 1.5484441301272984\n",
      "Log Reg (20 cls): 1.458981612446959\n"
     ]
    }
   ],
   "source": [
    "print(\"Linear Regression vs Logistic Regression (without regularization)\")\n",
    "test_y_5_pred = np.apply_along_axis(cls_5_to_value, 0, logReg_5_cls.predict(test_X))\n",
    "test_y_10_pred = np.apply_along_axis(cls_10_to_value, 0, logReg_10_cls.predict(test_X))\n",
    "test_y_20_pred = np.apply_along_axis(cls_20_to_value, 0, logReg_20_cls.predict(test_X))\n",
    "\n",
    "print(\"Test R^2\")\n",
    "print(\"Linear Reg:\", reg.score(test_X, test_y))\n",
    "print(\"Log Reg (5 cls):\", r2_score(test_y, test_y_5_pred))\n",
    "print(\"Log Reg (10 cls):\", r2_score(test_y, test_y_10_pred))\n",
    "print(\"Log Reg (20 cls):\", r2_score(test_y, test_y_20_pred))\n",
    "\n",
    "print(\"Test Accuracy\")\n",
    "lin_y_5_cls = np.apply_along_axis(to_5_classes, 0, reg.predict(test_X))\n",
    "lin_y_10_cls = np.apply_along_axis(to_10_classes, 0, reg.predict(test_X))\n",
    "lin_y_20_cls = np.apply_along_axis(to_20_classes, 0, reg.predict(test_X))\n",
    "print(\"Linear Reg (5 cls):\", accuracy_score(test_y_5_cls, lin_y_5_cls))\n",
    "print(\"Log Reg (5 cls):\", logReg_5_cls.score(test_X, test_y_5_cls))\n",
    "print(\"Linear Reg (10 cls):\", accuracy_score(test_y_10_cls, lin_y_10_cls))\n",
    "print(\"Log Reg (10 cls):\", logReg_10_cls.score(test_X, test_y_10_cls))\n",
    "print(\"Linear Reg (20 cls):\", accuracy_score(test_y_20_cls, lin_y_20_cls))\n",
    "print(\"Log Reg (20 cls):\", logReg_20_cls.score(test_X, test_y_20_cls))\n",
    "\n",
    "print(\"Test MSE\")\n",
    "print(\"Linear Reg:\", mean_squared_error(reg.predict(test_X), test_y))\n",
    "print(\"Log Reg (5 cls):\", mean_squared_error(test_y_5_pred, test_y_5_cls.apply(cls_5_to_value)))\n",
    "print(\"Log Reg (10 cls):\", mean_squared_error(test_y_10_pred, test_y_10_cls.apply(cls_10_to_value)))\n",
    "print(\"Log Reg (20 cls):\", mean_squared_error(test_y_20_pred, test_y_20_cls.apply(cls_20_to_value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression vs Logistic Regression (with l2 regularization)\n",
      "Test R^2\n",
      "Ridge Reg: 0.34131220992775924\n",
      "Log Reg L2 (5 cls): 0.05839047467655989\n",
      "Log Reg L2 (10 cls): 0.1763847544083318\n",
      "Log Reg L2 (20 cls): 0.18171903468553674\n",
      "Test Accuracy\n",
      "Ridge Reg (5 cls): 0.6294200848656294\n",
      "Log Reg L2 (5 cls): 0.632956152758133\n",
      "Ridge Reg (10 cls): 0.38189533239038187\n",
      "Log Reg L2 (10 cls): 0.38613861386138615\n",
      "Ridge Reg (20 cls): 0.20226308345120225\n",
      "Log Reg L2 (20 cls): 0.20332390381895332\n",
      "Test MSE\n",
      "Ridge Reg: 0.9824419425665801\n",
      "Log Reg L2 (5 cls): 1.7001414427157002\n",
      "Log Reg L2 (10 cls): 1.3426449787835926\n",
      "Log Reg L2 (20 cls): 1.2583097595473833\n"
     ]
    }
   ],
   "source": [
    "print(\"Ridge Regression vs Logistic Regression (with l2 regularization)\")\n",
    "test_y_5_pred = np.apply_along_axis(cls_5_to_value, 0, logRegL2_5_cls.predict(test_X))\n",
    "test_y_10_pred = np.apply_along_axis(cls_10_to_value, 0, logRegL2_10_cls.predict(test_X))\n",
    "test_y_20_pred = np.apply_along_axis(cls_20_to_value, 0, logRegL2_20_cls.predict(test_X))\n",
    "\n",
    "print(\"Test R^2\")\n",
    "print(\"Ridge Reg:\", ridgeReg.score(test_X, test_y))\n",
    "print(\"Log Reg L2 (5 cls):\", r2_score(test_y, test_y_5_pred))\n",
    "print(\"Log Reg L2 (10 cls):\", r2_score(test_y, test_y_10_pred))\n",
    "print(\"Log Reg L2 (20 cls):\", r2_score(test_y, test_y_20_pred))\n",
    "\n",
    "print(\"Test Accuracy\")\n",
    "lin_y_5_cls = np.apply_along_axis(to_5_classes, 0, ridgeReg.predict(test_X))\n",
    "lin_y_10_cls = np.apply_along_axis(to_10_classes, 0, ridgeReg.predict(test_X))\n",
    "lin_y_20_cls = np.apply_along_axis(to_20_classes, 0, ridgeReg.predict(test_X))\n",
    "print(\"Ridge Reg (5 cls):\", accuracy_score(test_y_5_cls, lin_y_5_cls))\n",
    "print(\"Log Reg L2 (5 cls):\", logRegL2_5_cls.score(test_X, test_y_5_cls))\n",
    "print(\"Ridge Reg (10 cls):\", accuracy_score(test_y_10_cls, lin_y_10_cls))\n",
    "print(\"Log Reg L2 (10 cls):\", logRegL2_10_cls.score(test_X, test_y_10_cls))\n",
    "print(\"Ridge Reg (20 cls):\", accuracy_score(test_y_20_cls, lin_y_20_cls))\n",
    "print(\"Log Reg L2 (20 cls):\", logRegL2_20_cls.score(test_X, test_y_20_cls))\n",
    "\n",
    "print(\"Test MSE\")\n",
    "print(\"Ridge Reg:\", mean_squared_error(ridgeReg.predict(test_X), test_y))\n",
    "print(\"Log Reg L2 (5 cls):\", mean_squared_error(test_y_5_pred, test_y_5_cls.apply(cls_5_to_value)))\n",
    "print(\"Log Reg L2 (10 cls):\", mean_squared_error(test_y_10_pred, test_y_10_cls.apply(cls_10_to_value)))\n",
    "print(\"Log Reg L2 (20 cls):\", mean_squared_error(test_y_20_pred, test_y_20_cls.apply(cls_20_to_value)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree & Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R^2:  0.4285439428914224\n",
      "Train MSE:  0.8440804787948721\n",
      "Valid R^2:  0.345017075863551\n",
      "Valid MSE:  0.9339722991508874\n",
      "Test R^2:  0.3863013694194297\n",
      "Test MSE:  0.9153399893930031\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dtreg = DecisionTreeRegressor(random_state = 0, max_depth = 8)\n",
    "dtreg.fit(train_X, train_y)\n",
    "\n",
    "print('Train R^2: ', dtreg.score(train_X, train_y))\n",
    "print('Train MSE: ', mean_squared_error(dtreg.predict(train_X), train_y))\n",
    "\n",
    "print('Valid R^2: ', dtreg.score(val_X, val_y))\n",
    "print('Valid MSE: ', mean_squared_error(dtreg.predict(val_X), val_y))\n",
    "\n",
    "print('Test R^2: ', dtreg.score(test_X, test_y))\n",
    "print('Test MSE: ', mean_squared_error(dtreg.predict(test_X), test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R^2:  0.8277948699701526\n",
      "Train MSE:  0.25435899540900153\n",
      "Valid R^2:  0.39379083619592503\n",
      "Valid MSE:  0.8644234003976554\n",
      "Test R^2:  0.44413131717729437\n",
      "Test MSE:  0.8290858230488398\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfreg = RandomForestRegressor(max_depth=32, random_state=0,n_estimators=100)\n",
    "rfreg.fit(train_X, train_y)\n",
    "\n",
    "print('Train R^2: ', rfreg.score(train_X, train_y))\n",
    "print('Train MSE: ', mean_squared_error(rfreg.predict(train_X), train_y))\n",
    "\n",
    "print('Valid R^2: ', rfreg.score(val_X, val_y))\n",
    "print('Valid MSE: ', mean_squared_error(rfreg.predict(val_X), val_y))\n",
    "\n",
    "print('Test R^2: ', rfreg.score(test_X, test_y))\n",
    "print('Test MSE: ', mean_squared_error(rfreg.predict(test_X), test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.682355616189177\n",
      "Train MSE:  1.4391390025769288\n",
      "Valid Accuracy:  0.6467468175388967\n",
      "Valid MSE:  1.5233380480905234\n",
      "Test Accuracy:  0.6538189533239038\n",
      "Test MSE:  1.5205091937765205\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# decision tree 5 classess\n",
    "dt_5_cls = DecisionTreeClassifier(random_state = 0, max_depth = 8).fit(train_X, train_y_5_cls)\n",
    "\n",
    "print('Train Accuracy: ', dt_5_cls.score(train_X, train_y_5_cls))\n",
    "print('Train MSE: ', mean_squared_error(np.apply_along_axis(cls_5_to_value, 0, dt_5_cls.predict(train_X)), train_y_5_cls.apply(cls_5_to_value)))\n",
    "\n",
    "print('Valid Accuracy: ', dt_5_cls.score(val_X, val_y_5_cls))\n",
    "print('Valid MSE: ', mean_squared_error(np.apply_along_axis(cls_5_to_value, 0, dt_5_cls.predict(val_X)), val_y_5_cls.apply(cls_5_to_value)))\n",
    "\n",
    "print('Test Accuracy: ', dt_5_cls.score(test_X, test_y_5_cls))\n",
    "print('Test MSE: ', mean_squared_error(np.apply_along_axis(cls_5_to_value, 0, dt_5_cls.predict(test_X)), test_y_5_cls.apply(cls_5_to_value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.4653630438077914\n",
      "Train MSE:  1.0494164013945733\n",
      "Valid Accuracy:  0.4169024045261669\n",
      "Valid MSE:  1.1014851485148516\n",
      "Test Accuracy:  0.423974540311174\n",
      "Test MSE:  1.0466760961810466\n"
     ]
    }
   ],
   "source": [
    "# decision tree 10 classess\n",
    "dt_10_cls = DecisionTreeClassifier(random_state = 0, max_depth = 8).fit(train_X, train_y_10_cls)\n",
    "\n",
    "print('Train Accuracy: ', dt_10_cls.score(train_X, train_y_10_cls))\n",
    "print('Train MSE: ', mean_squared_error(np.apply_along_axis(cls_10_to_value, 0, dt_10_cls.predict(train_X)), train_y_10_cls.apply(cls_10_to_value)))\n",
    "\n",
    "print('Valid Accuracy: ', dt_10_cls.score(val_X, val_y_10_cls))\n",
    "print('Valid MSE: ', mean_squared_error(np.apply_along_axis(cls_10_to_value, 0, dt_10_cls.predict(val_X)), val_y_10_cls.apply(cls_10_to_value)))\n",
    "\n",
    "print('Test Accuracy: ', dt_10_cls.score(test_X, test_y_10_cls))\n",
    "print('Test MSE: ', mean_squared_error(np.apply_along_axis(cls_10_to_value, 0, dt_10_cls.predict(test_X)), test_y_10_cls.apply(cls_10_to_value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.2788388661512809\n",
      "Train MSE:  1.0224533879035926\n",
      "Valid Accuracy:  0.22595473833097596\n",
      "Valid MSE:  1.0235148514851484\n",
      "Test Accuracy:  0.2256011315417256\n",
      "Test MSE:  1.0346534653465347\n"
     ]
    }
   ],
   "source": [
    "# decision tree 20 classess\n",
    "dt_20_cls = DecisionTreeClassifier(random_state = 0, max_depth = 8).fit(train_X, train_y_20_cls)\n",
    "\n",
    "print('Train Accuracy: ', dt_20_cls.score(train_X, train_y_20_cls))\n",
    "print('Train MSE: ', mean_squared_error(np.apply_along_axis(cls_20_to_value, 0, dt_20_cls.predict(train_X)), train_y_20_cls.apply(cls_20_to_value)))\n",
    "\n",
    "print('Valid Accuracy: ', dt_20_cls.score(val_X, val_y_20_cls))\n",
    "print('Valid MSE: ', mean_squared_error(np.apply_along_axis(cls_20_to_value, 0, dt_20_cls.predict(val_X)), val_y_20_cls.apply(cls_20_to_value)))\n",
    "\n",
    "print('Test Accuracy: ', dt_20_cls.score(test_X, test_y_20_cls))\n",
    "print('Test MSE: ', mean_squared_error(np.apply_along_axis(cls_20_to_value, 0, dt_20_cls.predict(test_X)), test_y_20_cls.apply(cls_20_to_value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.807340974384335\n",
      "Train MSE:  0.34129916233225344\n",
      "Valid Accuracy:  0.2950526704558547\n",
      "Valid MSE:  1.2123397734570602\n",
      "Test Accuracy:  0.36139665309725555\n",
      "Test MSE:  1.1348940652603077\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# random forest 5 classess\n",
    "rf_5_cls = RandomForestRegressor(max_depth=32, random_state=0,n_estimators=100).fit(train_X, train_y_5_cls)\n",
    "\n",
    "print('Train Accuracy: ', rf_5_cls.score(train_X, train_y_5_cls))\n",
    "print('Train MSE: ', mean_squared_error(np.apply_along_axis(cls_5_to_value, 0, rf_5_cls.predict(train_X)), train_y_5_cls.apply(cls_5_to_value)))\n",
    "\n",
    "print('Valid Accuracy: ', rf_5_cls.score(val_X, val_y_5_cls))\n",
    "print('Valid MSE: ', mean_squared_error(np.apply_along_axis(cls_5_to_value, 0, rf_5_cls.predict(val_X)), val_y_5_cls.apply(cls_5_to_value)))\n",
    "\n",
    "print('Test Accuracy: ', rf_5_cls.score(test_X, test_y_5_cls))\n",
    "print('Test MSE: ', mean_squared_error(np.apply_along_axis(cls_5_to_value, 0, rf_5_cls.predict(test_X)), test_y_5_cls.apply(cls_5_to_value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.8357200285743208\n",
      "Train MSE:  0.2572622672975503\n",
      "Valid Accuracy:  0.3739665811060906\n",
      "Valid MSE:  0.9502188073078852\n",
      "Test Accuracy:  0.41943151152941704\n",
      "Test MSE:  0.9218411010570399\n"
     ]
    }
   ],
   "source": [
    "#random forest with 10 classes\n",
    "rf_10_cls = RandomForestRegressor(max_depth=32, random_state=0,n_estimators=100).fit(train_X, train_y_10_cls)\n",
    "\n",
    "print('Train Accuracy: ', rf_10_cls.score(train_X, train_y_10_cls))\n",
    "print('Train MSE: ', mean_squared_error(np.apply_along_axis(cls_10_to_value, 0, rf_10_cls.predict(train_X)), train_y_10_cls.apply(cls_10_to_value)))\n",
    "\n",
    "print('Valid Accuracy: ', rf_10_cls.score(val_X, val_y_10_cls))\n",
    "print('Valid MSE: ', mean_squared_error(np.apply_along_axis(cls_10_to_value, 0, rf_10_cls.predict(val_X)), val_y_10_cls.apply(cls_10_to_value)))\n",
    "\n",
    "print('Test Accuracy: ', rf_10_cls.score(test_X, test_y_10_cls))\n",
    "print('Test MSE: ', mean_squared_error(np.apply_along_axis(cls_10_to_value, 0, rf_10_cls.predict(test_X)), test_y_10_cls.apply(cls_10_to_value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.6412691646069492\n",
      "Train MSE:  0.5370981301891746\n",
      "Valid Accuracy:  0.3895101761621004\n",
      "Valid MSE:  0.8834431021446513\n",
      "Test Accuracy:  0.43320073004051596\n",
      "Test MSE:  0.8558925643931317\n"
     ]
    }
   ],
   "source": [
    "# random forest 20 classess\n",
    "rf_20_cls = RandomForestRegressor(max_depth=16, random_state=0,n_estimators=100).fit(train_X, train_y_20_cls)\n",
    "\n",
    "print('Train Accuracy: ', rf_20_cls.score(train_X, train_y_20_cls))\n",
    "print('Train MSE: ', mean_squared_error(np.apply_along_axis(cls_20_to_value, 0, rf_20_cls.predict(train_X)), train_y_20_cls.apply(cls_20_to_value)))\n",
    "\n",
    "print('Valid Accuracy: ', rf_20_cls.score(val_X, val_y_20_cls))\n",
    "print('Valid MSE: ', mean_squared_error(np.apply_along_axis(cls_20_to_value, 0, rf_20_cls.predict(val_X)), val_y_20_cls.apply(cls_20_to_value)))\n",
    "\n",
    "print('Test Accuracy: ', rf_20_cls.score(test_X, test_y_20_cls))\n",
    "print('Test MSE: ', mean_squared_error(np.apply_along_axis(cls_20_to_value, 0, rf_20_cls.predict(test_X)), test_y_20_cls.apply(cls_20_to_value)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regression vs classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test R^2\n",
      "Reg: 0.3863013694194297\n",
      "Cls (5 cls): 0.18043880741900764\n",
      "Cls (10 cls): 0.35694421481363126\n",
      "Cls (20 cls): 0.32453549864056874\n",
      "Test Accuracy\n",
      "Reg (5 cls): 0.6534653465346535\n",
      "Cls (5 cls): 0.6538189533239038\n",
      "Reg (10 cls): 0.41937765205091937\n",
      "Cls (10 cls): 0.423974540311174\n",
      "Reg (20 cls): 0.22277227722772278\n",
      "Cls (20 cls): 0.2256011315417256\n",
      "Test MSE\n",
      "Reg: 0.9153399893930031\n",
      "Cls (5 cls): 1.5205091937765205\n",
      "Cls (10 cls): 1.0466760961810466\n",
      "Cls (20 cls): 1.0346534653465347\n"
     ]
    }
   ],
   "source": [
    "test_y_5_pred = np.apply_along_axis(cls_5_to_value, 0, dt_5_cls.predict(test_X))\n",
    "test_y_10_pred = np.apply_along_axis(cls_10_to_value, 0, dt_10_cls.predict(test_X))\n",
    "test_y_20_pred = np.apply_along_axis(cls_20_to_value, 0, dt_20_cls.predict(test_X))\n",
    "\n",
    "print(\"Test R^2\")\n",
    "print(\"Reg:\", dtreg.score(test_X, test_y))\n",
    "print(\"Cls (5 cls):\", r2_score(test_y, test_y_5_pred))\n",
    "print(\"Cls (10 cls):\", r2_score(test_y, test_y_10_pred))\n",
    "print(\"Cls (20 cls):\", r2_score(test_y, test_y_20_pred))\n",
    "\n",
    "print(\"Test Accuracy\")\n",
    "dt_y_5_cls = np.apply_along_axis(to_5_classes, 0, dtreg.predict(test_X))\n",
    "dt_y_10_cls = np.apply_along_axis(to_10_classes, 0, dtreg.predict(test_X))\n",
    "dt_y_20_cls = np.apply_along_axis(to_20_classes, 0, dtreg.predict(test_X))\n",
    "print(\"Reg (5 cls):\", accuracy_score(test_y_5_cls, dt_y_5_cls))\n",
    "print(\"Cls (5 cls):\", dt_5_cls.score(test_X, test_y_5_cls))\n",
    "print(\"Reg (10 cls):\", accuracy_score(test_y_10_cls, dt_y_10_cls))\n",
    "print(\"Cls (10 cls):\", dt_10_cls.score(test_X, test_y_10_cls))\n",
    "print(\"Reg (20 cls):\", accuracy_score(test_y_20_cls, dt_y_20_cls))\n",
    "print(\"Cls (20 cls):\", dt_20_cls.score(test_X, test_y_20_cls))\n",
    "\n",
    "print(\"Test MSE\")\n",
    "print(\"Reg:\", mean_squared_error(dtreg.predict(test_X), test_y))\n",
    "print(\"Cls (5 cls):\", mean_squared_error(test_y_5_pred, test_y_5_cls.apply(cls_5_to_value)))\n",
    "print(\"Cls (10 cls):\", mean_squared_error(test_y_10_pred, test_y_10_cls.apply(cls_10_to_value)))\n",
    "print(\"Cls (20 cls):\", mean_squared_error(test_y_20_pred, test_y_20_cls.apply(cls_20_to_value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regression vs Random Forest Classification\n",
      "Test R^2\n",
      "Reg: 0.44413131717729437\n",
      "Cls (5 cls): 0.4326914561301335\n",
      "Cls (10 cls): 0.43817476475853623\n",
      "Cls (20 cls): 0.4407802807085459\n",
      "Test Accuracy\n",
      "Reg (5 cls): 0.6630127298444131\n",
      "Cls (5 cls): 0.36139665309725555\n",
      "Reg (10 cls): 0.4282178217821782\n",
      "Cls (10 cls): 0.41943151152941704\n",
      "Reg (20 cls): 0.2362093352192362\n",
      "Cls (20 cls): 0.43320073004051596\n",
      "Test MSE\n",
      "Reg: 0.8290858230488398\n",
      "Cls (5 cls): 0.8461485339101295\n",
      "Cls (10 cls): 0.8379701033064753\n",
      "Cls (20 cls): 0.8340839402563981\n"
     ]
    }
   ],
   "source": [
    "print(\"Random Forest Regression vs Random Forest Classification\")\n",
    "test_y_5_pred = np.apply_along_axis(cls_5_to_value, 0, rf_5_cls.predict(test_X))\n",
    "test_y_10_pred = np.apply_along_axis(cls_10_to_value, 0, rf_10_cls.predict(test_X))\n",
    "test_y_20_pred = np.apply_along_axis(cls_20_to_value, 0, rf_20_cls.predict(test_X))\n",
    "\n",
    "print(\"Test R^2\")\n",
    "print(\"Reg:\", rfreg.score(test_X, test_y))\n",
    "print(\"Cls (5 cls):\", r2_score(test_y, test_y_5_pred))\n",
    "print(\"Cls (10 cls):\", r2_score(test_y, test_y_10_pred))\n",
    "print(\"Cls (20 cls):\", r2_score(test_y, test_y_20_pred))\n",
    "\n",
    "print(\"Test Accuracy\")\n",
    "rf_y_5_cls = np.apply_along_axis(to_5_classes, 0, rfreg.predict(test_X))\n",
    "rf_y_10_cls = np.apply_along_axis(to_10_classes, 0, rfreg.predict(test_X))\n",
    "rf_y_20_cls = np.apply_along_axis(to_20_classes, 0, rfreg.predict(test_X))\n",
    "print(\"Reg (5 cls):\", accuracy_score(test_y_5_cls, rf_y_5_cls))\n",
    "print(\"Cls (5 cls):\", rf_5_cls.score(test_X, test_y_5_cls))\n",
    "print(\"Reg (10 cls):\", accuracy_score(test_y_10_cls, rf_y_10_cls))\n",
    "print(\"Cls (10 cls):\", rf_10_cls.score(test_X, test_y_10_cls))\n",
    "print(\"Reg (20 cls):\", accuracy_score(test_y_20_cls, rf_y_20_cls))\n",
    "print(\"Cls (20 cls):\", rf_20_cls.score(test_X, test_y_20_cls))\n",
    "\n",
    "print(\"Test MSE\")\n",
    "print(\"Reg:\", mean_squared_error(rfreg.predict(test_X), test_y))\n",
    "# print(\"Cls (5 cls):\", mean_squared_error(test_y_5_pred, test_y_5_cls.apply(cls_5_to_value)))\n",
    "# print(\"Cls (10 cls):\", mean_squared_error(test_y_10_pred, test_y_10_cls.apply(cls_10_to_value)))\n",
    "# print(\"Cls (20 cls):\", mean_squared_error(test_y_20_pred, test_y_20_cls.apply(cls_20_to_value)))\n",
    "print(\"Cls (5 cls):\", mean_squared_error(test_y_5_pred, test_y))\n",
    "print(\"Cls (10 cls):\", mean_squared_error(test_y_10_pred, test_y))\n",
    "print(\"Cls (20 cls):\", mean_squared_error(test_y_20_pred, test_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['runtime', 'documentary', 'drama', 'horror', 'action', 'animation',\n",
      "       'hue_sd', 'saturation_sd ', 'hue', 'saturation', 'brightness_sd',\n",
      "       'green_sd', 'blue_sd', 'red_sd ', 'blue', 'green', 'red', 'short',\n",
      "       'brightness', 'thriller'],\n",
      "      dtype='object')\n",
      "[0.12409527 0.11660363 0.07672377 0.07490631 0.02208307 0.01840106\n",
      " 0.01460248 0.01437474 0.01323208 0.01302269 0.01094135 0.01073552\n",
      " 0.0105217  0.01045344 0.00985334 0.00980698 0.00947798 0.00925705\n",
      " 0.00879831 0.00724447]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAFFCAYAAADijCboAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deZxkVXn/8c93hk1ZBUaibIOC6OAWHEERcCEqSBAXkE1FQoLGEE3cgolBRBMlosQtCgGVRQKIwQwwgvpDQQGRYViHxQzDNkhkWAREtoHn98c5Rd+uqeo6Vbe6e/r29/161au7bt1T91R111PnnnvOcxQRmJlZc82Y7AqYmdn4cqA3M2s4B3ozs4ZzoDczazgHejOzhnOgNzNrOAd6m9Yk/aOk4ye7HmbjSR5Hb4OSdCuwEfBkZfMLIuK3NZ/zLyPip/VqN/VIOgLYMiLePdl1sWZxi97q2iMi1qrcBg7ywyBplck8/qCmar1tanCgt6GTtK6kEyTdJelOSZ+TNDM/9nxJF0i6V9I9kr4nab382MnAZsDZkv4g6ROSXidpadvz3yrpz/LvR0g6U9Ipkh4E3jfW8TvU9QhJp+TfZ0sKSQdJukPS/ZI+IOmVkq6R9HtJX6+UfZ+kiyV9XdIDkm6UtEvl8edKmifpPkmLJf1V23Gr9f4A8I/APvm1X533O0jSDZIekrRE0vsrz/E6SUslfVTS3fn1HlR5/BmSviTptly/X0p6Rn7sVZIuya/pakmva3tdS/Ixb5F0QJ//AraScSvCxsN3gbuBLYE1gXOAO4BjAQGfBy4C1gF+ABwB/F1EvEfSTlS6bqoBaAx7AnsD7wVWB04d4/gltge2AnYG5gHnAX8GrApcKen7EXFhZd8zgQ2BdwD/LWmLiLgPOA24Dngu8ELgJ5JujogLutR7Q1bsurkb+HNgSa7PjyRdHhEL8+N/AqwLbAy8EThT0g8j4n7gaGAbYAfg/3Jdn5K0MXAu8J782nYBfiDphcAfga8Cr4yImyQ9B1i/8H2zlZRb9FbXD3Or8PeSfihpI+AtpMD9cETcDRwD7AsQEYsj4icR8VhELAO+DLy2Zh0ujYgfRsRTpC+Prscv9NmIeDQifgw8DPxXRNwdEXcCvwD+tLLv3cC/R8QTEXE6cBOwu6RNgdcA/5Cf6yrgeFJQX6HeEfFIp4pExLkRcXMkFwI/Bnaq7PIEcGQ+/nzgD8DWkmYAfwF8OCLujIgnI+KSiHgMeDcwPyLm52P/BFiQ3zeAp4AXS3pGRNwVEYv6eO9sJeQWvdX1tuqFU0nbkVq+d0lqbZ5BalGTvwi+QgpWa+fH7q9Zhzsqv28+1vEL/a7y+yMd7q9VuX9njB7RcBupBf9c4L6IeKjtsbld6t2RpN2ATwMvIL2OZwLXVna5NyKWV+7/MddvQ2AN4OYOT7s5sLekPSrbVgV+FhEPS9oH+BhwgqSLgY9GxI296morL7fobdjuAB4DNoyI9fJtnYjYJj/+r0AAL4mIdUitS1XKtw8De5gU3ADIfe2z2vaplul1/GHbWJVvFNI1ht/m2/qS1m577M4u9V7hvqTVSV1bRwMbRcR6wHxGv1/d3AM8Cjy/w2N3ACdX3p/1ImLNiPgCQEScHxFvBJ4D3Aj8Z8HxbCXmQG9DFRF3kboXviRpHUkz8gXYVvfM2qTuhQdyX/HH257id8DzKvd/A6whaXdJqwKfIvVnD3r8YXs28CFJq0raG3gRqVvkDuAS4POS1pD0UuBg4JQxnut3wOzc7QKwGum1LgOW59b9m0oqlbuxvg18OV8Uninp1fnL4xRgD0lvztvXyBd2N5G0kaQ9Ja1J+sL8A6krx6YwB3obD+8lBanrSd0yZ5JahwCfAbYFHiBdEPzvtrKfBz6V+/w/FhEPAB8k9W/fSWrhL2VsYx1/2C4jXbi9B/gXYK+IuDc/th8wm9S6Pwv4dI/5Ad/PP++VtDB3+3wIOIP0OvYnXRwu9TFSN8/lwH3AUcCM/CW0J2mUzzJSC//jpHgwA/hIrvN9pOsnf93HMW0l5AlTZgOS9D7SCKEdJ7suZmNxi97MrOEc6M3MGs5dN2ZmDecWvZlZwznQm5k13Eo3M3bDDTeM2bNnT3Y1zMymlCuuuOKeiGifTAishIF+9uzZLFiwYLKrYWY2pUi6rdtj7roxM2s4B3ozs4ZzoDczazgHejOzhnOgNzNrOAd6M7OGc6A3M2s4B3ozs4Zb6SZM1TX7sHP72v/WL+w+TjUxM1s5uEVvZtZwDvRmZg1XFOgl7SrpJkmLJR3W4fGdJS2UtFzSXpXtL5d0qaRFkq6RtM8wK29mZr31DPSSZgLfAHYD5gD7SZrTttvtwPuAU9u2/xF4b0RsA+wK/Luk9epW2szMypVcjN0OWBwRSwAknUZaQf761g4RcWt+7KlqwYj4TeX330q6G5gF/L52zc3MrEhJ183GwB2V+0vztr5I2g5YDbi537JmZja4CbkYK+k5wMnAQRHxVIfHD5G0QNKCZcuWTUSVzMymjZJAfyewaeX+JnlbEUnrAOcC/xQRv+q0T0QcFxFzI2LurFkdF0gxM7MBlQT6y4GtJG0haTVgX2BeyZPn/c8CToqIMwevppmZDapnoI+I5cChwPnADcAZEbFI0pGS3gog6ZWSlgJ7A8dKWpSLvwvYGXifpKvy7eXj8krMzKyjohQIETEfmN+27fDK75eTunTay50CnFKzjmZmVkPjct3U4Tw5ZtZEToFgZtZwDvRmZg3nQG9m1nAO9GZmDedAb2bWcA70ZmYN50BvZtZwDvRmZg3nQG9m1nAO9GZmDedAb2bWcA70ZmYN50BvZtZwDvRmZg3nQG9m1nAO9GZmDedAb2bWcA70ZmYN50BvZtZwDvRmZg3nQG9m1nAO9GZmDedAb2bWcA70ZmYNVxToJe0q6SZJiyUd1uHxnSUtlLRc0l5tjx0o6X/z7cBhVdzMzMr0DPSSZgLfAHYD5gD7SZrTttvtwPuAU9vKrg98Gtge2A74tKRn1a+2mZmVKmnRbwcsjoglEfE4cBqwZ3WHiLg1Iq4Bnmor+2bgJxFxX0TcD/wE2HUI9TYzs0IlgX5j4I7K/aV5W4mispIOkbRA0oJly5YVPrWZmZVYKS7GRsRxETE3IubOmjVrsqtjZtYoJYH+TmDTyv1N8rYSdcqamdkQlAT6y4GtJG0haTVgX2Be4fOfD7xJ0rPyRdg35W1mZjZBegb6iFgOHEoK0DcAZ0TEIklHSnorgKRXSloK7A0cK2lRLnsf8FnSl8XlwJF5m5mZTZBVSnaKiPnA/LZth1d+v5zULdOp7LeBb9eoo5mZ1bBSXIw1M7Px40BvZtZwDvRmZg3nQG9m1nAO9GZmDedAb2bWcA70ZmYN50BvZtZwDvRmZg3nQG9m1nAO9GZmDedAb2bWcA70ZmYN50BvZtZwDvRmZg3nQG9m1nAO9GZmDedAb2bWcA70ZmYN50BvZtZwDvRmZg3nQG9m1nAO9GZmDedAb2bWcEWBXtKukm6StFjSYR0eX13S6fnxyyTNzttXlXSipGsl3SDpk8OtvpmZ9dIz0EuaCXwD2A2YA+wnaU7bbgcD90fElsAxwFF5+97A6hHxEuAVwPtbXwJmZjYxSlr02wGLI2JJRDwOnAbs2bbPnsCJ+fczgV0kCQhgTUmrAM8AHgceHErNzcysSEmg3xi4o3J/ad7WcZ+IWA48AGxACvoPA3cBtwNHR8R9NetsZmZ9GO+LsdsBTwLPBbYAPirpee07STpE0gJJC5YtWzbOVTIzm15KAv2dwKaV+5vkbR33yd006wL3AvsD50XEExFxN3AxMLf9ABFxXETMjYi5s2bN6v9VmJlZVyWB/nJgK0lbSFoN2BeY17bPPODA/PtewAUREaTumjcASFoTeBVw4zAqbmZmZXoG+tznfihwPnADcEZELJJ0pKS35t1OADaQtBj4CNAagvkNYC1Ji0hfGN+JiGuG/SLMzKy7VUp2ioj5wPy2bYdXfn+UNJSyvdwfOm03M7OJUxTorbfZh53bd5lbv7D7ONTEzGw0p0AwM2s4B3ozs4Zz181Kot+uH3f7mFkpt+jNzBrOgd7MrOEc6M3MGs6B3sys4RzozcwazoHezKzhHOjNzBrOgd7MrOEc6M3MGs6B3sys4RzozcwazoHezKzhHOjNzBrOgd7MrOEc6M3MGs6B3sys4RzozcwazoHezKzhHOjNzBrOgd7MrOEc6M3MGq4o0EvaVdJNkhZLOqzD46tLOj0/fpmk2ZXHXirpUkmLJF0raY3hVd/MzHrpGeglzQS+AewGzAH2kzSnbbeDgfsjYkvgGOCoXHYV4BTgAxGxDfA64Imh1d7MzHoqadFvByyOiCUR8ThwGrBn2z57Aifm388EdpEk4E3ANRFxNUBE3BsRTw6n6mZmVqIk0G8M3FG5vzRv67hPRCwHHgA2AF4AhKTzJS2U9IlOB5B0iKQFkhYsW7as39dgZmZjGO+LsasAOwIH5J9vl7RL+04RcVxEzI2IubNmzRrnKpmZTS8lgf5OYNPK/U3yto775H75dYF7Sa3/iyLinoj4IzAf2LZupc3MrFxJoL8c2ErSFpJWA/YF5rXtMw84MP++F3BBRARwPvASSc/MXwCvBa4fTtXNzKzEKr12iIjlkg4lBe2ZwLcjYpGkI4EFETEPOAE4WdJi4D7SlwERcb+kL5O+LAKYHxHnjtNrMTOzDnoGeoCImE/qdqluO7zy+6PA3l3KnkIaYmlmZpPAM2PNzBrOgd7MrOEc6M3MGs6B3sys4RzozcwazoHezKzhHOjNzBrOgd7MrOEc6M3MGs6B3sys4RzozcwazoHezKzhHOjNzBrOgd7MrOEc6M3MGs6B3sys4RzozcwazoHezKzhHOjNzBrOgd7MrOEc6M3MGs6B3sys4RzozcwazoHezKzhigK9pF0l3SRpsaTDOjy+uqTT8+OXSZrd9vhmkv4g6WPDqbaZmZXqGeglzQS+AewGzAH2kzSnbbeDgfsjYkvgGOCotse/DPyofnXNzKxfJS367YDFEbEkIh4HTgP2bNtnT+DE/PuZwC6SBCDpbcAtwKLhVNnMzPpREug3Bu6o3F+at3XcJyKWAw8AG0haC/gH4DNjHUDSIZIWSFqwbNmy0rqbmVmB8b4YewRwTET8YaydIuK4iJgbEXNnzZo1zlUyM5teVinY505g08r9TfK2TvsslbQKsC5wL7A9sJekfwPWA56S9GhEfL12zc3MrEhJoL8c2ErSFqSAvi+wf9s+84ADgUuBvYALIiKAnVo7SDoC+IODvJnZxOoZ6CNiuaRDgfOBmcC3I2KRpCOBBRExDzgBOFnSYuA+0peBmZmtBEpa9ETEfGB+27bDK78/Cuzd4zmOGKB+ZmZWk2fGmpk1nAO9mVnDOdCbmTWcA72ZWcM50JuZNZwDvZlZwznQm5k1nAO9mVnDOdCbmTWcA72ZWcM50JuZNZwDvZlZwznQm5k1nAO9mVnDOdCbmTWcA72ZWcM50JuZNZwDvZlZwznQm5k1nAO9mVnDOdCbmTWcA72ZWcM50JuZNZwDvZlZw61SspOkXYGvADOB4yPiC22Prw6cBLwCuBfYJyJulfRG4AvAasDjwMcj4oIh1t+A2Yed29f+t35h93GqiZmtjHq26CXNBL4B7AbMAfaTNKdtt4OB+yNiS+AY4Ki8/R5gj4h4CXAgcPKwKm5mZmVKum62AxZHxJKIeBw4DdizbZ89gRPz72cCu0hSRFwZEb/N2xcBz8itfzMzmyAlgX5j4I7K/aV5W8d9ImI58ACwQds+7wQWRsRjg1XVzMwGUdRHX5ekbUjdOW/q8vghwCEAm2222URUycxs2ihp0d8JbFq5v0ne1nEfSasA65IuyiJpE+As4L0RcXOnA0TEcRExNyLmzpo1q79XYGZmYypp0V8ObCVpC1JA3xfYv22feaSLrZcCewEXRERIWg84FzgsIi4eXrVtmDxqx6zZerboc5/7ocD5wA3AGRGxSNKRkt6adzsB2EDSYuAjwGF5+6HAlsDhkq7Kt2cP/VWYmVlXRX30ETEfmN+27fDK748Ce3co9zngczXraGZmNXhmrJlZwznQm5k1nAO9mVnDOdCbmTXchEyYsuaqMzSz37Lt5c2sjFv0ZmYN5xa9TVkTeTbhMwmbytyiNzNrOAd6M7OGc6A3M2s499Gb9anuaCFfW7CJ5kBvNo34i2J6cqA3syKTOWfCX1D1ONCbWaN5Up8vxpqZNZ4DvZlZw7nrxsxsnKws1xbcojczazgHejOzhnOgNzNrOAd6M7OGc6A3M2s4B3ozs4ZzoDczazgHejOzhisK9JJ2lXSTpMWSDuvw+OqSTs+PXyZpduWxT+btN0l68/CqbmZmJXoGekkzgW8AuwFzgP0kzWnb7WDg/ojYEjgGOCqXnQPsC2wD7Ar8R34+MzObICUt+u2AxRGxJCIeB04D9mzbZ0/gxPz7mcAukpS3nxYRj0XELcDi/HxmZjZBFBFj7yDtBewaEX+Z778H2D4iDq3sc13eZ2m+fzOwPXAE8KuIOCVvPwH4UUSc2XaMQ4BD8t2tgZvqv7QVbAjcMwllJ/PYrvf0ObbrPb2O3cnmETGr0wMrRVKziDgOOG48jyFpQUTMneiyk3ls13v6HNv1nl7H7ldJ182dwKaV+5vkbR33kbQKsC5wb2FZMzMbRyWB/nJgK0lbSFqNdHF1Xts+84AD8+97ARdE6hOaB+ybR+VsAWwF/Ho4VTczsxI9u24iYrmkQ4HzgZnAtyNikaQjgQURMQ84AThZ0mLgPtKXAXm/M4DrgeXA30TEk+P0Wnqp0zVUt1tpso7tek+fY7ve0+vYfel5MdbMzKY2z4w1M2s4B3ozs4ZzoDczazgH+i4mK1WDpBmS3jUZxzbrl6RnTnYd+iFp75JtQz7mDEk7jOcxetahyRdjJW0E/Cvw3IjYLefeeXVEnFBQdgnwA+A7EXH9AMfenZTjZ43Wtog4srBsnYkYryHNSN6cNKpK6dDxvMLyOwCzqYzIioiTepT5GtD1HykiPlRy7LokbQ5sFRE/lfQMYJWIeKiw7ExgI0a/7tt7lBn4ddd9zyS9Y6zHI+K/x3q8rvx/cjywVkRsJullwPsj4oPjeMyPjPV4RHy54DkWRsS2vbZ1Kbsm8EhEPCXpBcALSTP9nygoe2VE/Gmv/cbLSjEzdhx9F/gO8E/5/m+A00nDQXt5GWmY6PGSZgDfJuXtebBXQUnfAp4JvJ70YdiL/uYP/FTSx3JdH25tjIj7CsqeAPw9cAXQ11BWSScDzweuqpQNYMxADyzIP19DSnx3er6/N2lo7biT9FekNBrrk17DJsC3gF0Kyv4t8Gngd8BTeXMAL+1RtM7rrvue7ZF/PhvYAbgg3389cAkwroGelLzwzeQ5NRFxtaSdSwrmL6mjSHUXI42RdXoUXTv/3Bp4JSPzefagx+dL0m7AW4CNJX218tA6pKHfJS4CdpL0LODHpDlG+wAHFJT9f5LeCfx3TEbrOiIaewMuzz+vrGy7aoDneS1pRu/DpORtW/bY/5q2n2sBv+jjeLd0uC0pLHtZjffrBvJZ3oDlf0VqRbfur0rKdTRWmbNJH9iOtz6OfRWwWtvf+trCsouBDSbydQ+jbN7/x8BzKvefA5w/xv7rj3Xr47iX5Z/V9/vqPt7vF9V4vy8C1q7cXxu4qEeZl5Emdd6Wf7Zu7wCeVXjchfnn3wKfaP3fFZZ9iNSIeAJ4MN9/cND3oN9b01v0D0vagHyKLOlVwAMlBfOp/O7AQaSujC8B3wN2AuYDLxij+CP55x8lPZeUDuI5pZWOiC1K9+3gZ5K+SGrRPVZ5zoUFZa8D/gS4a8BjP4vUQmqdeayVt43l6PzzHfnYp+T7+5Fa2KUei4jHU9LUp1NxlLac7qDw/6KLQV73MMoCbBoR1b/X74DNxtj/CtL7orzf/fn39YDbgdL/vTty901IWhX4MKmhUOJ3EVG6bycbAY9X7j+et3UV6YzjOuDNEXHiWPuOQZJeTWrBH5y3FV3Li4i1e+81fpoe6D9Cahk+X9LFwCxSN0qJ/wV+BnwxIi6pbD+z4BT1HEnrAV8EFpI+WMeXVjp/cP4aaB3n58CxUdAXSMoaClDt4w/gDQVlNwSul/RrRn9JvLWgLMAXgCsl/YwUPHYmXS/oKiIuBJD0pRh9XeJsSQu6FOvkQkn/CDxD0huBD5LOFkosAX4u6VxGv+6efb5Z3697SGUhdQmcD/xXvr8v8NNuO7caEZL+EzgrIubn+7sBb+vjuB8AvgJsTDrb/THwN4VlF0g6Hfgho9/v0u6mk4BfSzor338bI2nSu4qIJyVtKmm1SCnX+/V3wCdJ79siSc8jxYiectr2A4AtIuKzkjYlnYlNSEqYRl+MhadbdluTPkQ3lQTL3Jr/pyi8eNrjuVYH1oiI4hajpONJp/Ctf973AE9GThU9XiS9ttP2VjAufI4/YeTL5rKI+L/CcjcAu0fEknx/C2B+RLyosPwMUivrTaS/9fnA8VHwDy7p0522R8RnSo6dn2Og1123bC7/dkYaBRdFxFlj7Z/LXBsRL+m1bTxI+k6HzRERf9HHc2xLOruG9JqvLCx3EvAiUgOwev2r9Eu99TwzSBeie16zy/t/k9R184aIeFGrnz8iXtnPcQfV6EBf6X6ZzejRFCVX538dEQMtklLnuLn81RHxsl7bupRdl3RhsfXBvxA4svSLJo9Uav3z/Toi7i4pl8u+htRn+bCkdwPbAl+JiNsKyu5Kyv+xhBSoNwcOiYgflx6/LklrAUTEH/osV+d1D1w2l18TeDS3VrcmNWp6jgTJZwG/YKSr7ABg54goWu4zB+sVgkc/wboOSTuSRlh9R9IsUtC9paDcwF/qkk4lnck8SboQuw7pb/XFgrILI2Lb6uib0s/0MDS96+Zs4FHgWkZGU5S6WNLXWXHkS0lfd53jAjwp6fkRcTNAPkUsHUHzbVJfe2ss/ntII4/GHI6Xj/MuUnfTz0nB9muSPh5tC8WM4ZvAy/JQu4+QRgCdRLqYPaaIOE/SVqQhawA3RsRjY5Vpq/stdA48PYeVSnoxcDLpgiSS7gHeGxGLCg8/8OuuWRZGjwQ5jzSap2QkyH6kBkGr9X9h3lbqnMrvawBvB35bUjAPTfwmsFFEvFjSS4G3RsTnCst/mtQ1uTXpf3tV0hfWa3qVbQX0Ab/U50TEg5IOAH4EHEa65tEz0ANP5AZg63rhLAaLDYOZqKu+k3Ejj3oZsOzPOtwuGO/j5vJvIF0Y+znpA3gr8PrCsiuMAui0rUvZq4FnV+7PonAkRd6/NSrhcODg6raCsnuTR1IAnyJdTN62j2NvULltTOpPPbKw7CXV9xd4HXDJBL3ugcu2le97JEjlOWYC69T8n51R+p7l/+ntGD1i57o+jnUVqSFSLV/0mQNeDFxJGn1zGylQb1NYdhHpS+X7wGvzttKRRgeQuouWAv9CWkVv7zrveV9/n4k60GTcSGN13zSVjps/dH8PrE4ax/1SYPU+yl8K7Fi5/xrg0sKy17bdn9G+rUf5C0kXq35DGkFTXJ6Roag7kr5Ud6fGUNH8XFcU7rfCh7X0AzyE1z1w2Vz+SuDVpGGa23T6O3Ypdyqp62FN0rj9pcDHa7zXW5PWli7Zt9awZ1KXIox8ya3ZR6Af+Esd+BDpwvN8RroX+xk2/ULSBetDqTG8dJBb07tufgWclS+cPEH5xAyg1uzWgY8bqa91v4g4BrimpJ5t/ho4MffVizRs732FZc9rG8GxD+mfutQ+wP6klun/SdqMstNaGOma2h34z4g4V1LRqTw8fXGuZQbp1L70/3uJpH8mdd8AvJt0raBUndddpyykYY2DjASp0w2BpIcYGaYZwP8B/1BY53skPZ+Rboy9KBzSm0evnCPpWGA9pYlyfwH8Z+Gx14yIp9+fiPh5vs7RU0R8FahOtrpN0ut71Hf9yt27GflsIWn9KJsEWVvTL8beAuxJauH09ULVZXZrRBw8ZsGax83ljyGdIg5yfaD1HOvkMkWjAirl3slIX+cvomAERx/PfWlEvLrLY+eQWktvJF2QfIT0fhddrMrDE1vv9XJSd9fREfGbgrLPAj5DOpuAdJHyiIi4v+TYBc/f9XWPZ9lc/msR8bcdti8CXk5q2X89Ii6cqIuD+cvoONKM3vtJEwLfHRG3Fpa/lnQ94+kRVhHxk8KyZ5GGPFe/1F8REW8vKNt3SpXKtSNVNj/9BRmFqUnqanqgvwh4XUT0fdFD0jUR8dLKz7VIoxl2Kig78HFz+U4tsoiIrmPhJb07Ik5Rl3wg0efwsfGgMfJ9KCXH2pX05fi/kp4DvCTyqBtJzxor8EpaA3gno0c6ReEZ2Lga63WPZ9lcvmMeF0kfIrXAryadRW0GnNLr/7vtzGkFfTZG1gRmRGE+okq5E0lfTpf3Uy6XHfhLXdKPyClVIuJlSkO3r4wJGJJaV9O7bloTYX5E/xNh6sxuHfi4+cr8vNx104/W6WenGXhjfptL+mVE7Fg5HX/6Ifro6irQtR4R8Ucq+Vkizfasns7/P1JLv5sfAr8ntdYeLamMpH+PiL+TdHanukX5RLFe6rSmxqUl1t4NIel20tlr6/6B0XkG6Ze61K3VhdNzYt4gLeM22wMHSLqN0We8vXITkQP6oEn2NoyIMyR9Mj/XckljjoYb5hdjHU0P9Lfk22r51o86s1sHPm6rj56UNKqfcsfmX38aERdXH8tjtccqu2P+OanTtHtQj8c3iYhd+3zO1un70WPuNQ3kLsZqcq8P02G2aUS8HkApO+gHSS3jILWMv1l4uO8yeLJBSMnUBpKHdn6MFee4lMwcHySlypfGeKzoi3EYGt11U4ek1SOP41ae3UqamFI8trvGsQfuo+90qt7t9L1D2ZMj4j29tg2qZhfGmK9B0nHA1yLi2gGe+8MR8ZVe2wY1yV03A5XvVU7SGaTkXN/Lm/YH1o2InmspSLo8Il6p0ZOHroqIl/dbz35JupqU1XRUdteIuKKg7LbA10hDNK8jp1SJiEEGTUyoRrboh3RKfim5qyAH98ckLWTs7oPW8WcBn2DFETul37X50VgAABFRSURBVN6tf/hq//KY3/5KyZZ2AGa19dOvQ2HiJVJ9q8+5CvCKwrKtMpvTPSf8UL4w2o53Lem9WQU4SGkdgccY6XbqeTpPymLYHtTf12Fbtzr0ylNe53UXlZX0zNz91W7QL6teLcAXR8Scyv2fSSpNST1wssEhWB4RpWceo0TEQqU0IcUpVSS9ISIuUJf1A2Kc1w1oaWSgp8YpuVLekY1JybH+lJEug3VIo3BKfI/UGv9z0pTpA4FlpXVonR73aTVS5sNVGN1P/yA9ErnlPsdWQrDWKB2RsgIeV1oB9cgJHxHXlT5Xp6fvsv3PB37C1EW2P7CFpHmVh9ZmJJtkiTHzlI/1utUjN3uv90yVBUCAFRYAiYjv9vE6Rj11j8cXSnpVRPwq12N7RnLs91In2eBANDLM8WxJHyTNCK5ePyv9e2/HSLfPtpKIsRfmeS1prYA9OjwWjP+6AflIEzhof6JvwIdLtrU9fiBpHPJDjJ4VOw94R+Fxr8g/r6lsu7yPeq8LfJn0wVlA6udbt7Ds5jXer8/XfL8Hzgmf950JPJc0AmQzYLPKY8W50vs43uakCTOXkj6Qrdu2VHLEFzxPnTzldXOzXwZsyoCzTMd43q932X4taX7HDaQp/LeSrkc9BVxf+Df+e1Kg3IbUDbLqsP+2HY57C2mQxC2V25LWrfA5TiZNuPoPUhfO14CvFpSbAbxrvF/jWLemtuhb+j4ljzTS4ERJ74yIHwx43Nbp3F1Kk65+S86jUmjgfDWkFbH2jojfw9PDyU6LgmRVEfHJvP9WjO5yuqiw3gPnhFePVZ5iHCaWREocdhtpZmkd0oB5yqmfm52IuKP1nmddR4J0G35bea4v55+Hdtll4DOo/LzVCYGluYRqi5H0zO8Czos0WeyfSV/qny18mrmkiWZ9XdiM1KX3CeCMfsoNUyMD/ZBOyc+RtD8rXp0vGZf9OaWZqR8lfeuvQ2rFlHp+RLyzcv8zkq4qLLthK8hDGk4m6dklBSX9JWm0xSak1vmrSK3d0msLF2rwnPAfBraOiHsL9x+a3Ef8NVL62tVIQfrhKB9WOnCecurnZu93AZCBl+PL9SrKqtlDnYSBdX0q0hDJHUn/10eTRgttP3YxoN7CPHWWB62tkYGedHp1F2khjerwpocoTyvwP6QLRFdQ+QD2ksfBbxUR5+Tyg/S3PyJpx4j4ZX7O1zAyrr+XpyRtFnlha0mzKR+L/WHSh/9XEfF6SS8kjXcudRipRXst8H5S+oTSIal1V3mq4+ukBTu+T2q1vZexVxAbJVK+/tYCKjOAe6J8QfR1gD+SZnk+/ZSU9932tQBIjGRvvIiUNO6hfP8I4NzCY9bVGmzQSg1cPAZ/COqk2qizMM8++Wf1bxOAZ8ZOJknXRcSLByw7cC77XP7lpDHM6+ZN9wMHRsEwLo3kdb+Q9AHaiZTX/fyCsq1hb1cB20fEY5IWRcQ2vcrWJekEUitz0FWe6hx7QUTMVZ4FnbcVD0tUjTzlk0XSTcBLY/QQ4msiYusJOPZHGZ0WIEiDBhZEROmZ66DHHjjVhoawMM9kaWqLHug9oqGHSyS9JAYYl039U9MbgH8jjVxZj9TSfRsFZyOR8rrPJY1+uZLUJVB6NrBUaZLYD4GfSLqf1IddRDVywpPSMt/OYJPb6vqjpNWAqyT9G+lscEYf5QdOEKb6udm/w2ALgAy0HN+QvIJ05jSP9Jn8c9L/9vslfT8i/m0cj/0uUqqNoyPi90qpNj5eWPYtETEqcZuko8hnc73kLrbZjO4KHmvEztA0ukUvaTGwxyAXu/KY4C1JV+f7GpetkVw1rTe3Vbbo1FTSeYxM569O6hhrll2rbMd+9tJjV57ntaQzivOicH3NPDa6ZQ1Sjvn1I+LwPo470CpPdeSx/78jfcH8Pel1/0dELC4sP3CCMEkXkgLNsTEyeaj4bFIpCV3L0wuAlHQdacDl+OrK3UZvaf2N89/8XFIAviJGj89faajzZMRrCmPCyaSG21WMfKajjy6+WhrdoqfeiIbd+i1QGdFwDp0z1pUaZDp/S61+9jzqZlPS9YyHSMPfis5EOlxI/XdJV5AW1eh13LqrPA0kX1P514g4gJQjp3id2IpjScMMrwYuyl8cpVlDnxkRv24bNbO8287t2keGSfov4JelxwYejLwcn6QtomA5viF4NqOvez1BOqN5RNK4zzzvl6S/Jg0seJ6k6ln12qTrgSUGGrEzLE0P9AOPaIiI29RhXcoexdpHNPwPKdgXjWioqNNt9GhEPCqplcbhRqW1RHuS9FnS8NMljB7iWHomUicn/HHARyLnCpf0OlKO8R0Kyw8kD/fbXNJqpWcuHZ6j7zzlFQPnZu9iK1IgHZNqLMc3BN8DLpP0P/n+HsCpSjOMS2fXTqRTSV1ynyd1y7U81MeomTojdmpretfNdzpsjoL+y1EfhIh4gVIGy+9HRM8PQj413b0yomFt4NyI2LlHuep0/q1IAbffbqOzgINIQ/7eQLqQu2pEvKWg7E2k1MADBTzVywk/8ILodUk6iTS0ch6jr6mULuY+cDZG1c/N3mkBkE+2t/Q7lLsK+FPSZK9Wl1FRN8Qw5OtIrc/SxRFROqt2UuUzwI0Y3c9++xj7t9KwrE3q3htkxE5tjW7RR8RBNYq/nfxByM/12xywS2xESh/Q8nje1kutySgAMbKAwhE58K5LWjS6xHWki793D3j43VgxJ/y+jM7Z003dVZ7quDnfZtA5zXMv32XAbIwRsQT4Mw2Ymz0Gzzj6eESEpNaZRNEqS8OSA/uUCO4tkg4FjqDLpL4ujiZ9CR9FuuD99NPlbROi0YG+xogEqPdB6DSi4bu9CsVwJqNUn6/fYV+fB66UdB2DtTr6zglf8Rek/vFWt9ov8rZx1xpbXkPfecpbBj0bUP0852do8OX4pqu/o89Jfa3PoKRV2z+PSkn/JkSjAz3pomjL0yMSCssO/EGIiH9RWnSkNaLhoIka0VDTiaRWxrWMtFj6MfBF5Ki3IEQtbV1OT+tjpFKdbIzfZbCzgYEXAFG68ns6Kcvmg6R++sOjcDm+aazvSX09LuRe3LnU8DW6j76d0qzFX0ZE0QU+pWn8fa9LOVUpT5iqUb7vnPCauFWexqpDNRVza0nC5RHxicLyA+cpV83c7OqyAEhEjHlGJenamAJL4K0MKqPptqHPSX1KqVCeRb0LubU1vUXfrmhEQktE/ETSZeT3SRO4avsk+YWkz5MuSlb/kcfsBlC9nPCTvspTrLjoxMVK09xLy/edp7yibm72E0mt8taon/1JXYe9FgBZKOmVMcC6q9NQ6zpI35P6IuIB0t9zv/GpWpnGtujz6emTQHXiTdGIhFz+/aQ+40dJ3RgTumr7ZNAAi5LncpuP9XjJtQeN8ypPPY5dzSw6gzRz86vRRzoADTjrsc7ZQC5/ffsEo07bOpS7kTQhsLXuaj8LtdgU09gWfb6Qen0MmK+GtK7kiyPinmHWa2UWgy14MqyLyLVWearpCkaGKC4nDXE8eMwSFeoy65HUsh6r3ExGcuAPcjYAgy8AMvC6q9NVl+7FB0jv97G9ussmU2MDfXZFjdPTm0lZBRtP0rsj4hR1yVVeOp58wGMPa5WnOl7U/iFVSvJVatA85QPnZq90l61KmmB3e76/OXBjwbGHOsJrmlhCOuP6r3x/H9Ls8ReQBmoMfanMYWl6oN8eOEDSIKennyR9gC5jdH/1pIwMGWetoaODjsmuYxgppYdRh/bhipd22NZNnVmPgybAqz3nwvq2Q9tghbMrF9MnbBGVQTQ90Nc5PT2WtNbjoEMNp4yIODb/rDuefJBjD2uVp75pOOsDQ7085QPlZneLfFKspdFrPWzGSFqUgWaTT5RGB/qaH4ZVI2LMZdeaRimfz1+x4kXFcZ+4pPqrPA3izaTrAJuQ1uhteYi0WHqpI2rUoT0BXgAPSnp5jHNuduvbR4FfSrqZ9PfaAvhgnkw5USmeB9LYUTd1SfpXUq6WsxlstfgpR9IlpHHYVzA6PfKga+f2c+wFdFjlKSI+OQHHrrM+MJKOig55ytu3dSl7Kp1zs88m5VYaz9zs1qd87eaF+e5NK/MF2CoH+i6UFtFo1/ThlcUTdcbh2LVWeRrC8XcnTYipLopekqOnbp7yKZmbfTqR9IaIuEBpIaMVRPn6vpOm0V03dUReNX6aOUfSWyJi/iQcu+4qTwOT9C1Sn/zrSWvc7kVBWuke09tL85RPqdzs09RrSdfr9ujwWD/r+04at+i7kPTeTttLJsFMVUopb9ckBZ4noK+lF+seu9YqTzWPfU1EvLTycy3gRxGxU49ytae3K2XsfDtp7QJIwWQeaQTScZEWRLFJltOn7BURZ0x2XQbhQN+FpK9V7q4B7ELK3b3XJFVpQuRZolsxugtjXBc/zhOHTpqsoKa8mLukXwHvII3fvy4ituzjOfrKU95WdkrmZp9uWt2Lk12PQbjrpouI+NvqfaVFs0+bpOpMCHVeb/YS0pfcuIkhrPJU09n57/tFUorloI+UvRosT/nTYgrmZp+mfirpY6w452GlH6DhQF/uYdJwqiartd5sTUtIk4cGWuWpphuBJyPiB0r54Lcl5dYv1XeecpuS9iF9gX+wbftKP0DDgb6LtrwWM4A5wJTsn+vDwOvNDkHdVZ7q+OeI+L7SGsFvIGXS/CZpZnWJvvOU25Q0hxVTQn9rUmtUyH30XeS0sy3LgdsiYulk1WciqMZ6s1NZaxhnTtF8bUScWjK0s06ecpt6JJ1BSgn9vbxpf2DdiOiVEnrSOdB3IWkL4K7WhIi8wMNGUbho81SXv+jWBc6biH5z1V/lqc6xzwHuBN5I6rZ5BPh19FiYXGkB+a4mI6WEjZ9BU0KvDBzou8gzNXdoBbk8xvviqLECk3Wnmqs81Tz2M0kTlK6NiP+V9BzgJRHx4/E+tk0dkk4Bvt6WEvpvIqLjUOyViQN9F51miUq6ulcrz4anNexxsuvRy1TOU269taWE3pq0ytTTKaGnQoveF2O7WybprRExD0DSnsC0WYRkonVZ5WndSapOv6ZsnnIrMuVTQrtF34Wk55MuumycN90BvCcibp68WjVXzi3UvsrTkRHxy0mtWAF1WFS9mqc8IraZrLqZgVv0XeWA/qo8HZ5W0ikbN3VXeZpMUzZPuU0PDvRd5DwmnwZ2zvcvJLUwPV56fNRd5WkyTdk85TY9uOumC0k/IC0R1/qgvgd4WUR0TFVqg6ms8nQKaVxydZWnb0XEC7uVXZlM1TzlNj040HfRZdTNpOVrbypJB5JWeZrL6HwvDwHfXZlzfTchT7lND+666e4RSTu2LgZKeg1pIo0NUUScCJxYd5WnSTLl85Tb9OAWfReSXgacxMgQv/uBAyPimu6lrI46qzxNlqmep9ymBwf6NpX8JZD6i9fMvz9MWoTD+UvGQbdVniLi4EmtWIGpnKfcpocJWaptilk73+YCHyBdFFwXeD9TYwTIVLVDnkp+f84R82rShKOp4KeSPiZpU0nrt26TXSmzFrfou8iLNu8eEQ/l+2sD50bEzpNbs2aSdFlEbF9Z5eleYFE/qzxNlspkr1GavJC8TS2+GNvdRoye7PJ43mbj45wOqzwdP7lVKjZl85Tb9OAWfReS/gl4F3BW3vQ24PSI+Pzk1Wp6yGPS15gqk9Omcp5ymx4c6McgaVtgp3z3ooi4cjLr02SS9iblvn9I0qdI10M+OxXe86mcp9ymB3fdjCEiFpK6EWz8VZfz+zNSF863KF/ObzItlPSqtjzlXuzbVhoO9LayeDL/3B04LiLOlfS5yaxQL215yi+RNCpP+WTWzazKXTe2Uhh0Ob/JJGnzsR6PiNsmqi5mY3Ggt5WCl/MzGz8O9GZmDeeZsWZmDedAb2bWcA70ZmYN50BvZtZwDvRmZg33/wHHvJuvFVxJ7wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "importances_order = np.argsort(rfreg.feature_importances_)[::-1]\n",
    "print(train_X.columns[importances_order[:20]])\n",
    "print(rfreg.feature_importances_[importances_order[:20]])\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(20),rfreg.feature_importances_[importances_order[:20]])\n",
    "plt.xticks(range(20), train_X.columns[importances_order[:20]], rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R^2:  0.6506230970382416\n",
      "Train MSE:  0.5160540690109419\n",
      "Valid R^2:  0.38669584852279837\n",
      "Valid MSE:  0.8745404915542743\n",
      "Test R^2:  0.4384435177634063\n",
      "Test MSE:  0.8375692544852968\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "xgreg = xgb.XGBRegressor(objective ='reg:squarederror')\n",
    "xgreg.fit(train_X, train_y)\n",
    "\n",
    "print('Train R^2: ', xgreg.score(train_X, train_y))\n",
    "print('Train MSE: ', mean_squared_error(xgreg.predict(train_X), train_y))\n",
    "\n",
    "print('Valid R^2: ', xgreg.score(val_X, val_y))\n",
    "print('Valid MSE: ', mean_squared_error(xgreg.predict(val_X), val_y))\n",
    "\n",
    "print('Test R^2: ', xgreg.score(test_X, test_y))\n",
    "print('Test MSE: ', mean_squared_error(xgreg.predict(test_X), test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.8343944217068364\n",
      "Train MSE:  0.7315446415037138\n",
      "Valid Accuracy:  0.6403818953323904\n",
      "Valid MSE:  1.5855728429985856\n",
      "Test Accuracy:  0.6584158415841584\n",
      "Test MSE:  1.502121640735502\n"
     ]
    }
   ],
   "source": [
    "# XGBoost 5 classes\n",
    "xgb_5_cls = xgb.XGBClassifier(objective ='multi:softmax').fit(train_X, train_y_5_cls)\n",
    "\n",
    "print('Train Accuracy: ', xgb_5_cls.score(train_X, train_y_5_cls))\n",
    "print('Train MSE: ', mean_squared_error(np.apply_along_axis(cls_5_to_value, 0, xgb_5_cls.predict(train_X)), train_y_5_cls.apply(cls_5_to_value)))\n",
    "\n",
    "print('Valid Accuracy: ', xgb_5_cls.score(val_X, val_y_5_cls))\n",
    "print('Valid MSE: ', mean_squared_error(np.apply_along_axis(cls_5_to_value, 0, xgb_5_cls.predict(val_X)), val_y_5_cls.apply(cls_5_to_value)))\n",
    "\n",
    "print('Test Accuracy: ', xgb_5_cls.score(test_X, test_y_5_cls))\n",
    "print('Test MSE: ', mean_squared_error(np.apply_along_axis(cls_5_to_value, 0, xgb_5_cls.predict(test_X)), test_y_5_cls.apply(cls_5_to_value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.7816431711383962\n",
      "Train MSE:  0.4185235713202971\n",
      "Valid Accuracy:  0.413012729844413\n",
      "Valid MSE:  1.1739745403111739\n",
      "Test Accuracy:  0.41336633663366334\n",
      "Test MSE:  1.1488684582743989\n"
     ]
    }
   ],
   "source": [
    "# XGBoost 10 classes\n",
    "xgb_10_cls = xgb.XGBClassifier(objective ='multi:softmax').fit(train_X, train_y_10_cls)\n",
    "\n",
    "print('Train Accuracy: ', xgb_10_cls.score(train_X, train_y_10_cls))\n",
    "print('Train MSE: ', mean_squared_error(np.apply_along_axis(cls_10_to_value, 0, xgb_10_cls.predict(train_X)), train_y_10_cls.apply(cls_10_to_value)))\n",
    "\n",
    "print('Valid Accuracy: ', xgb_10_cls.score(val_X, val_y_10_cls))\n",
    "print('Valid MSE: ', mean_squared_error(np.apply_along_axis(cls_10_to_value, 0, xgb_10_cls.predict(val_X)), val_y_10_cls.apply(cls_10_to_value)))\n",
    "\n",
    "print('Test Accuracy: ', xgb_10_cls.score(test_X, test_y_10_cls))\n",
    "print('Test MSE: ', mean_squared_error(np.apply_along_axis(cls_10_to_value, 0, xgb_10_cls.predict(test_X)), test_y_10_cls.apply(cls_10_to_value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.8081703804759739\n",
      "Train MSE:  0.24238290131878126\n",
      "Valid Accuracy:  0.22065063649222066\n",
      "Valid MSE:  1.134016973125884\n",
      "Test Accuracy:  0.22772277227722773\n",
      "Test MSE:  1.091495756718529\n"
     ]
    }
   ],
   "source": [
    "# XGBoost 20 classes\n",
    "xgb_20_cls = xgb.XGBClassifier(objective ='multi:softmax').fit(train_X, train_y_20_cls)\n",
    "\n",
    "print('Train Accuracy: ', xgb_20_cls.score(train_X, train_y_20_cls))\n",
    "print('Train MSE: ', mean_squared_error(np.apply_along_axis(cls_20_to_value, 0, xgb_20_cls.predict(train_X)), train_y_20_cls.apply(cls_20_to_value)))\n",
    "\n",
    "print('Valid Accuracy: ', xgb_20_cls.score(val_X, val_y_20_cls))\n",
    "print('Valid MSE: ', mean_squared_error(np.apply_along_axis(cls_20_to_value, 0, xgb_20_cls.predict(val_X)), val_y_20_cls.apply(cls_20_to_value)))\n",
    "\n",
    "print('Test Accuracy: ', xgb_20_cls.score(test_X, test_y_20_cls))\n",
    "print('Test MSE: ', mean_squared_error(np.apply_along_axis(cls_20_to_value, 0, xgb_20_cls.predict(test_X)), test_y_20_cls.apply(cls_20_to_value)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Regression vs XGBoost Classification\n",
      "Test R^2\n",
      "Reg: 0.4384435177634063\n",
      "Cls (5 cls): 0.19343074338304445\n",
      "Cls (10 cls): 0.2970580282348778\n",
      "Cls (20 cls): 0.2937863363315256\n",
      "Test Accuracy\n",
      "Reg (5 cls): 0.6697312588401697\n",
      "Cls (5 cls): 0.6584158415841584\n",
      "Reg (10 cls): 0.43316831683168316\n",
      "Cls (10 cls): 0.41336633663366334\n",
      "Reg (20 cls): 0.22984441301272984\n",
      "Cls (20 cls): 0.22772277227722773\n",
      "Test MSE\n",
      "Reg: 0.8375692544852968\n",
      "Cls (5 cls): 1.502121640735502\n",
      "Cls (10 cls): 1.1488684582743989\n",
      "Cls (20 cls): 1.091495756718529\n"
     ]
    }
   ],
   "source": [
    "print(\"XGBoost Regression vs XGBoost Classification\")\n",
    "test_y_5_pred = np.apply_along_axis(cls_5_to_value, 0, xgb_5_cls.predict(test_X))\n",
    "test_y_10_pred = np.apply_along_axis(cls_10_to_value, 0, xgb_10_cls.predict(test_X))\n",
    "test_y_20_pred = np.apply_along_axis(cls_20_to_value, 0, xgb_20_cls.predict(test_X))\n",
    "\n",
    "print(\"Test R^2\")\n",
    "print(\"Reg:\", xgreg.score(test_X, test_y))\n",
    "print(\"Cls (5 cls):\", r2_score(test_y, test_y_5_pred))\n",
    "print(\"Cls (10 cls):\", r2_score(test_y, test_y_10_pred))\n",
    "print(\"Cls (20 cls):\", r2_score(test_y, test_y_20_pred))\n",
    "\n",
    "print(\"Test Accuracy\")\n",
    "xg_y_5_cls = np.apply_along_axis(to_5_classes, 0, xgreg.predict(test_X))\n",
    "xg_y_10_cls = np.apply_along_axis(to_10_classes, 0, xgreg.predict(test_X))\n",
    "xg_y_20_cls = np.apply_along_axis(to_20_classes, 0, xgreg.predict(test_X))\n",
    "print(\"Reg (5 cls):\", accuracy_score(test_y_5_cls, xg_y_5_cls))\n",
    "print(\"Cls (5 cls):\", xgb_5_cls.score(test_X, test_y_5_cls))\n",
    "print(\"Reg (10 cls):\", accuracy_score(test_y_10_cls, xg_y_10_cls))\n",
    "print(\"Cls (10 cls):\", xgb_10_cls.score(test_X, test_y_10_cls))\n",
    "print(\"Reg (20 cls):\", accuracy_score(test_y_20_cls, xg_y_20_cls))\n",
    "print(\"Cls (20 cls):\", xgb_20_cls.score(test_X, test_y_20_cls))\n",
    "\n",
    "print(\"Test MSE\")\n",
    "print(\"Reg:\", mean_squared_error(xgreg.predict(test_X), test_y))\n",
    "print(\"Cls (5 cls):\", mean_squared_error(test_y_5_pred, test_y_5_cls.apply(cls_5_to_value)))\n",
    "print(\"Cls (10 cls):\", mean_squared_error(test_y_10_pred, test_y_10_cls.apply(cls_10_to_value)))\n",
    "print(\"Cls (20 cls):\", mean_squared_error(test_y_20_pred, test_y_20_cls.apply(cls_20_to_value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "work"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
